{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T00:10:29.424040Z",
     "start_time": "2019-02-09T00:10:27.730905Z"
    },
    "code_folding": [],
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter, namedtuple\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Initializing notebook. Please wait...', file=sys.stderr)\n",
    "\n",
    "import esper.captions as captions\n",
    "from esper.major_canonical_shows import MAJOR_CANONICAL_SHOWS\n",
    "from esper.widget import *\n",
    "from esper.rekall import *\n",
    "from rekall.interval_list import IntervalList\n",
    "\n",
    "WIDGET_STYLE_ARGS = {'description_width': 'initial'}\n",
    "\n",
    "ANCHOR_WORD_WINDOW_SIZE = 15\n",
    "CONTEXT_WORD_EXTEND_THRESH = 120\n",
    "\n",
    "\n",
    "def merge_postings(p1, p2):\n",
    "    start_idx = min(p1.idx, p2.idx)\n",
    "    end_idx = max(p1.idx + p1.len, p2.idx + p2.len)\n",
    "    return p1._replace(\n",
    "        start=min(p1.start, p2.start), \n",
    "        end=max(p1.end, p2.end), \n",
    "        idx=start_idx,\n",
    "        len=end_idx - start_idx)\n",
    "\n",
    "def extend_postings(postings, threshold):\n",
    "    merged = []\n",
    "    curr_p = None\n",
    "    for p in postings:\n",
    "        if curr_p is None:\n",
    "            curr_p = p\n",
    "        elif p.start >= curr_p.start and p.start - curr_p.end <= threshold:\n",
    "            curr_p = merge_postings(curr_p, p)\n",
    "        else:\n",
    "            merged.append(curr_p)\n",
    "            curr_p = p\n",
    "    else:\n",
    "        merged.append(curr_p)\n",
    "    return merged\n",
    "\n",
    "def extend_postings_with_context(anchors, contexts, threshold):\n",
    "    results = []\n",
    "    for anchor_p in anchors:\n",
    "        for context_p in contexts:\n",
    "            if context_p.start >= anchor_p.start and context_p.start - anchor_p.end <= threshold:\n",
    "                anchor_p = merge_postings(anchor_p, context_p)\n",
    "        for context_p in contexts[::-1]:\n",
    "            if context_p.start <= anchor_p.start and anchor_p.start - context_p.end <= threshold:\n",
    "                anchor_p = merge_postings(anchor_p, context_p)\n",
    "        results.append(anchor_p)\n",
    "    return extend_postings(results, threshold)\n",
    "\n",
    "\n",
    "TopicSegments = namedtuple('TopicSegments', [\n",
    "    'video_to_segments', 'video_to_anchor_words', 'video_to_context_words'\n",
    "])\n",
    "\n",
    "\n",
    "def find_segments(anchor_words, context_words):\n",
    "    print('Searching for segments...'.format(len(anchor_words), len(context_words)), \n",
    "          file=sys.stderr)\n",
    "    \n",
    "    # Find the anchor locations\n",
    "    video_anchor_locations = {}\n",
    "    for d in captions.topic_search(list(anchor_words), window_size=ANCHOR_WORD_WINDOW_SIZE):\n",
    "        video_anchor_locations[d.id] = list(d.postings)\n",
    "    \n",
    "    # Search for context locations\n",
    "    video_context_locations = {}\n",
    "    for d in captions.topic_search(list(context_words), window_size=0, \n",
    "                                   video_ids=video_anchor_locations.keys()):\n",
    "        video_context_locations[d.id] = list(d.postings)\n",
    "    \n",
    "    # Extend the anchor locations\n",
    "    video_topic_segments = {}\n",
    "    for video_id, anchor_postings in video_anchor_locations.items():\n",
    "        video_topic_segments[video_id] = extend_postings_with_context(\n",
    "            anchor_postings, video_context_locations.get(video_id, []),\n",
    "            CONTEXT_WORD_EXTEND_THRESH)\n",
    "    \n",
    "    coverage_seconds = sum(sum(p.end - p.start for p in l) for l in video_topic_segments.values())\n",
    "    print('Found {} segments in {} videos covering {:0.2f} minutes.'.format(\n",
    "        sum(len(l) for l in video_topic_segments.values()),\n",
    "        len(video_topic_segments),\n",
    "        coverage_seconds / 60\n",
    "    ), file=sys.stderr)\n",
    "    return TopicSegments(video_topic_segments, video_anchor_locations, video_context_locations)\n",
    "\n",
    "MIN_TOKEN_COUNT = 10000\n",
    "\n",
    "def propose_context_words(topic_result, k=192, ncols=8, default_threshold=3.):\n",
    "    topic_word_counts = Counter()\n",
    "    for video_id, segments in topic_result.video_to_segments.items():\n",
    "        d = captions.get_document(video_id)\n",
    "        for p in segments:\n",
    "            topic_word_counts.update(captions.INDEX.tokens(d, p.idx, p.len))\n",
    "\n",
    "    all_words_total = sum(w.count for w in captions.LEXICON)\n",
    "    topic_words_total = sum(topic_word_counts.values())\n",
    "    \n",
    "    def filter_cond(t):\n",
    "        if t not in captions.LEXICON: \n",
    "            return False\n",
    "        w = captions.LEXICON[t]\n",
    "        return w.count > MIN_TOKEN_COUNT and w.token not in CONTEXT_WORDS\n",
    "\n",
    "    const_expr = math.log(all_words_total) - math.log(topic_words_total) \n",
    "    log_pmis = [\n",
    "        (t, math.log(topic_word_counts[t]) - math.log(captions.LEXICON[t].count) + const_expr)\n",
    "        for t in topic_word_counts.keys() if filter_cond(t)\n",
    "    ]\n",
    "    log_pmis.sort(key=lambda x: -x[1])\n",
    "    log_pmis = log_pmis[:k]\n",
    "    \n",
    "    selections = []\n",
    "    for t, score in log_pmis:\n",
    "        token = captions.LEXICON[t].token\n",
    "        w = widgets.ToggleButton(\n",
    "            value=score >= default_threshold,\n",
    "            description=token,\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "            icon=''\n",
    "        )\n",
    "        selections.append((t, w))\n",
    "    \n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    def on_submit(b):\n",
    "        selected_words = []\n",
    "        for t, w in selections:\n",
    "            if w.value == True:\n",
    "                selected_words.append(captions.LEXICON[t].token)\n",
    "        clear_output()\n",
    "        print('Added {} words to the context.'.format(len(selected_words)))\n",
    "        \n",
    "        global CONTEXT_WORDS\n",
    "        CONTEXT_WORDS.update(selected_words)\n",
    "        sync_context_widget()\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    hboxes = []\n",
    "    for i in range(0, len(selections), ncols):\n",
    "        hboxes.append(widgets.HBox([w for _, w in selections[i:i + ncols]]))\n",
    "    vbox = widgets.VBox(hboxes)\n",
    "    display(widgets.HBox([\n",
    "        widgets.Label(\n",
    "            'Instructions: Select new context words and hit submit. '\n",
    "            '(Likely words may already be highlighted.) '),\n",
    "        submit_button\n",
    "    ]))\n",
    "    display(vbox)\n",
    "    \n",
    "def display_segments(topic_results, filters={}, limit=1000, results_per_page=50):\n",
    "    video_to_topic_time = {\n",
    "        video_id : sum(p.end - p.start for p in postings)\n",
    "        for video_id, postings in topic_results.video_to_segments.items()\n",
    "    }\n",
    "    video_qs = Video.objects.filter(id__in=list(video_to_topic_time.keys()), duplicate=False)\n",
    "    if 'show' in filters:\n",
    "        video_qs = video_qs.filter(show__canonical_show__name=filters['show'])\n",
    "    if 'channel' in filters:\n",
    "        video_qs = video_qs.filter(channel__name=filters['channel'])\n",
    "    if 'start' in filters:\n",
    "        video_qs = video_qs.filter(time__gte=filters['start'])\n",
    "    if 'end' in filters:\n",
    "        video_qs = video_qs.filter(time__lte=filters['end'])\n",
    "    video_to_fps = {\n",
    "        v['id']: v['fps'] for v in video_qs.values('id', 'fps', 'channel__name')\n",
    "    }\n",
    "    if len(video_to_fps) == 0:\n",
    "        print('No videos to display', file=sys.stderr)\n",
    "        return\n",
    "    video_to_topic_time = {k: v for k, v in video_to_topic_time.items() if k in video_to_fps}\n",
    "    limit_video_ids = set(sorted(video_to_fps, key=lambda x: -video_to_topic_time[x])[:limit])\n",
    "    \n",
    "    def convert_time(v, t):\n",
    "        return int(t * video_to_fps[v])\n",
    "    def to_intervallist(video_to_postings):\n",
    "        return {\n",
    "            video_id : IntervalList([\n",
    "                (convert_time(video_id, p.start), convert_time(video_id, p.end), None)\n",
    "                for p in postings\n",
    "            ]) \n",
    "            for video_id, postings in video_to_postings.items() \n",
    "            if video_id in limit_video_ids\n",
    "        }\n",
    "    \n",
    "    # Plot distribution of topic times in videos\n",
    "    def plot_dist_of_videos(results_per_page):\n",
    "        plt.figure(figsize=(7,2))\n",
    "        x = np.arange(len(video_to_topic_time))\n",
    "        y = np.array(sorted(video_to_topic_time.values(), key=lambda x: -x)) / 60\n",
    "        plt.plot(x, y, color='red')\n",
    "        plt.fill_betweenx([0, np.max(y)], len(limit_video_ids), alpha=0.2, color='gray')\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.ylim(0, np.max(y))\n",
    "        plt.xlabel('Video Number')\n",
    "        plt.xlim(0, len(video_to_topic_time))\n",
    "        plt.show()\n",
    "\n",
    "    print('Videos (ordered by descending segment time)')\n",
    "    plot_dist_of_videos(results_per_page)\n",
    "    print('Loading {} of {} videos (shaded region)... Please wait.'.format(\n",
    "        len(limit_video_ids), len(video_to_topic_time)))\n",
    "    \n",
    "    # Convert to intervallists\n",
    "    video_to_topic_intervals = to_intervallist(topic_results.video_to_segments)\n",
    "    video_to_anchor_intervals = to_intervallist(topic_results.video_to_anchor_words)\n",
    "    video_to_context_intervals = to_intervallist({\n",
    "        k: extend_postings(v, CONTEXT_WORD_EXTEND_THRESH) \n",
    "        # Coalesce context words to reduce memory usage\n",
    "        for k, v in topic_results.video_to_context_words.items()\n",
    "    })\n",
    "    video_to_commerical_intervals = qs_to_intrvllists(\n",
    "        Commercial.objects.filter(labeler__name='haotian-commercials',\n",
    "                                  video__id__in=limit_video_ids))\n",
    "    \n",
    "    # Display results\n",
    "    result = intrvllists_to_result(\n",
    "        video_to_topic_intervals, color='red',\n",
    "        video_order=list(sorted(\n",
    "            video_to_topic_intervals,\n",
    "            key=lambda x: -video_to_topic_time[x])))\n",
    "    add_intrvllists_to_result(result, video_to_anchor_intervals, color='blue')\n",
    "    add_intrvllists_to_result(result, video_to_context_intervals, color='orange')\n",
    "    add_intrvllists_to_result(result, video_to_commerical_intervals, color='black')\n",
    "    \n",
    "    display(esper_widget(result, jupyter_keybindings=True, results_per_page=results_per_page))\n",
    "    \n",
    "def is_uniformly_sampled(is_3y, fps, frame_number):\n",
    "    if is_3y:\n",
    "        return frame_number % math.floor(fps * 3) == 0\n",
    "    else:\n",
    "        return frame_number % math.ceil(fps * 3) == 0\n",
    "    \n",
    "def analysis(topic_results, n=10000):\n",
    "    video_to_meta = {\n",
    "        v['id']: {\n",
    "            'channel': v['channel__name'],\n",
    "            'show': v['show__canonical_show__name'],\n",
    "            'time': v['time'],\n",
    "            'fps': v['fps'],\n",
    "            'is_3y': v['threeyears_dataset'],\n",
    "            'path': v['path']\n",
    "        } for v in Video.objects.filter(\n",
    "            id__in=list(topic_results.video_to_segments.keys()), \n",
    "            duplicate=False\n",
    "        ).values(\n",
    "            'id', 'channel__name', 'show__canonical_show__name', 'time', 'fps',\n",
    "            'threeyears_dataset', 'path'\n",
    "        )\n",
    "    }\n",
    "    channels = [c.name for c in Channel.objects.all()]\n",
    "    utc = timezone('UTC')\n",
    "    eastern = timezone('US/Eastern')\n",
    "    \n",
    "    channel_to_time = {c: 0. for c in channels}\n",
    "    channel_to_daypart_to_time = {c: np.zeros(24) for c in channels}\n",
    "    channel_to_weekday_to_time = {c: np.zeros(7) for c in channels}\n",
    "    channel_to_time_to_time = {c: defaultdict(float) for c in channels}\n",
    "    show_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "\n",
    "        video_topic_len = sum(p.end - p.start for p in postings)\n",
    "        channel = video_to_meta[video_id]['channel']\n",
    "        channel_to_time[channel] += video_topic_len\n",
    "        \n",
    "        video_dt = utc.localize(video_to_meta[video_id]['time']).astimezone(eastern)\n",
    "        for p in postings:\n",
    "            base_hour = video_dt.hour\n",
    "            posting_len = p.end - p.start\n",
    "            channel_to_daypart_to_time[channel][\n",
    "                (base_hour + int(p.start / 3600)) % 24\n",
    "            ] += posting_len\n",
    "            \n",
    "        channel_to_weekday_to_time[channel][video_dt.weekday()] += video_topic_len\n",
    "        channel_to_time_to_time[channel][video_dt.date()] += video_topic_len\n",
    "        \n",
    "        show = video_to_meta[video_id]['show']\n",
    "        show_to_time[(channel, show)] += video_topic_len\n",
    "        \n",
    "    print('Topic time by channel:')\n",
    "    for c in channel_to_time:\n",
    "        print('  {}: {:0.3f} hours'.format(c, channel_to_time[c] / 3600))\n",
    "        \n",
    "    print('\\nTopic time by daypart:')\n",
    "    def plot_daypart():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(24) + (i - 1) * bar_width,\n",
    "                    channel_to_daypart_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(24))\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.show()\n",
    "    plot_daypart()\n",
    "    \n",
    "    print('\\nTopic time by weekday:')\n",
    "    def plot_weekday():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(7) + (i - 1) * bar_width, \n",
    "                    channel_to_weekday_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.show()\n",
    "    plot_weekday()\n",
    "    \n",
    "    print('\\nTopic time by day:')\n",
    "    def plot_timeline():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for c in channels:\n",
    "            data = [x for x in sorted(channel_to_time_to_time[c].items())]\n",
    "            plt.scatter(\n",
    "                [x for x, _ in data], [y / 60 for _, y in data],\n",
    "                alpha=0.5, s=2, label=c)\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Day')\n",
    "        plt.show()\n",
    "    plot_timeline()\n",
    "    \n",
    "    top_n = 10\n",
    "    print('\\nShows with most coverage (top-{}):'.format(top_n))\n",
    "    for (channel, show), seconds in show_to_time.most_common(top_n):\n",
    "        print('  {} ({}): {:0.1f} minutes'.format(show, channel, seconds / 60))\n",
    "        \n",
    "    # Time by Gender\n",
    "    print('Loading face genders...', file=sys.stderr)\n",
    "    print('Not implemented...', file=sys.stderr)\n",
    "#     video_to_face_genders = defaultdict(list)\n",
    "#     face_gender_qs = FaceGender.objects.filter(\n",
    "#         labeler=Labeler.objects.get(name='knn-gender'), \n",
    "#         face__frame__video__id__in=list(video_to_meta.keys()),\n",
    "#     )\n",
    "#     for f in face_gender_qs.values(\n",
    "#         'gender__name', 'face__frame__video__id', 'face__frame__number', 'probability'\n",
    "#     ).order_by('?')[:n]:\n",
    "#         video_id = f['face__frame__video__id']\n",
    "#         fps = video_to_meta[video_id]['fps']\n",
    "#         is_3y = video_to_meta[video_id]['is_3y']\n",
    "#         frame_number = f['face__frame__number']\n",
    "#         if not is_uniformly_sampled(is_3y, fps, frame_number):\n",
    "#             continue\n",
    "#         start = frame_number / fps\n",
    "#         video_to_face_genders[video_id].append({\n",
    "#             'label': f['gender__name'],\n",
    "#             'probability': f['probability'],\n",
    "#             'start': start,\n",
    "#             'end': start + 3\n",
    "#         })\n",
    "        \n",
    "    # Time by Identity\n",
    "    print('Loading face identities...', file=sys.stderr)\n",
    "    print('Not implemented...', file=sys.stderr)\n",
    "#     ident_labelers = [\n",
    "#         l['id'] for l in Labeler.objects.filter(\n",
    "#             name__contains='face-identity'\n",
    "#         ).exclude(name__contains='face-identity-old:').values('id')\n",
    "#     ]\n",
    "#     video_to_face_idents = defaultdict(list)\n",
    "#     face_ident_qs = FaceIdentity.objects.filter(\n",
    "#         labeler__id__in=ident_labelers, \n",
    "#         face__frame__video__id__in=list(video_to_meta.keys()),\n",
    "#         probability__gt=0.5\n",
    "#     )\n",
    "#     for f in face_ident_qs.values(\n",
    "#         'identity__name', 'face__frame__video__id', 'face__frame__number', 'probability'\n",
    "#     ).order_by('?')[:n]:\n",
    "#         video_id = f['face__frame__video__id']\n",
    "#         fps = video_to_meta[video_id]['fps']\n",
    "#         is_3y = video_to_meta[video_id]['is_3y']\n",
    "#         frame_number = f['frame__number']\n",
    "#         if not is_uniformly_sampled(is_3y, fps, frame_number):\n",
    "#             continue\n",
    "#         start = frame_number / fps\n",
    "#         video_to_face_idents[video_id].append({\n",
    "#             'label': f['identity__name'],\n",
    "#             'probability': f['probability'],\n",
    "#             'start': start,\n",
    "#             'end': start + 3\n",
    "#         })\n",
    "\n",
    "print('Done initializing notebook.', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Stories from a Lexicon\n",
    "\n",
    "Stories are retreived via lexicons of words. Story lexicons have two components, <b>anchor phrases</b> and <b>context phrases</b>.\n",
    "\n",
    "<b>Anchor phrases</b> are phrases that must appear for a segment to be considered a part of a story and should be unique to the story. For instance, a segment about 'Hurricane Irma' must mention either 'Irma' or 'Hurricane Irma'.\n",
    "\n",
    "<b>Context phrases</b> are phrases that are used to describe the story and are used to determine that time extent of a topic segment. For instance, words such as 'devastation' and 'storm' around an anchor such as 'Hurricane Irma' indicate that the discussion is part of the story. These phrases need not be unique to the story.\n",
    "\n",
    "<b>Instructions:</b>\n",
    "- Enter anchor phrases to start (required)\n",
    "- Enter a few context phrases (optional, but recommended)\n",
    "- Hit 'search for segments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T00:10:29.560524Z",
     "start_time": "2019-02-09T00:10:29.426579Z"
    },
    "hide_input": true,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ANCHOR_WORDS\n",
    "    CONTEXT_WORDS\n",
    "    TOPIC_SEGMENTS\n",
    "except NameError:\n",
    "    ANCHOR_WORDS = set()\n",
    "    CONTEXT_WORDS = set()\n",
    "    TOPIC_SEGMENTS = None\n",
    "\n",
    "status_output = widgets.Output()\n",
    "anchor_widget = widgets.Textarea(\n",
    "    style=WIDGET_STYLE_ARGS,\n",
    "    value='',\n",
    "    layout=widgets.Layout(width='100%'),\n",
    "    placeholder='Phrases (one per line)',\n",
    "    description='Anchor phrases:',\n",
    "    disabled=False\n",
    ")\n",
    "def sync_anchor_widget():\n",
    "    anchor_widget.value = '\\n'.join(sorted(ANCHOR_WORDS))\n",
    "    anchor_widget.layout = widgets.Layout(\n",
    "        width='100%', \n",
    "        height='{}px'.format(20 * (len(ANCHOR_WORDS) + 2))\n",
    "    )\n",
    "def on_anchor_changed(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        try:\n",
    "            global ANCHOR_WORDS\n",
    "            ANCHOR_WORDS = {t.strip() for t in anchor_widget.value.split('\\n') if len(t.strip()) > 0}\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "anchor_widget.observe(on_anchor_changed, names='value')\n",
    "\n",
    "context_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    style=WIDGET_STYLE_ARGS,\n",
    "    layout=widgets.Layout(width='100%'),\n",
    "    placeholder='Phrases (one per line)',\n",
    "    description='Context phrases:',\n",
    "    disabled=False\n",
    ")\n",
    "def sync_context_widget():\n",
    "    context_widget.value = '\\n'.join(sorted(CONTEXT_WORDS))\n",
    "    context_widget.layout = widgets.Layout(\n",
    "        width='100%', \n",
    "        height='{}px'.format(20 * (len(CONTEXT_WORDS) + 2))\n",
    "    )\n",
    "def on_context_changed(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        try:\n",
    "            global CONTEXT_WORDS\n",
    "            CONTEXT_WORDS = {t.strip() for t in context_widget.value.split('\\n') if len(t.strip()) > 0}\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "context_widget.observe(on_context_changed, names='value')\n",
    "\n",
    "sort_button = widgets.Button(\n",
    "    description='Sort phrases',\n",
    "    disabled=False,\n",
    "    button_style=''\n",
    ")\n",
    "def on_sort(b):\n",
    "    sync_anchor_widget()\n",
    "    sync_context_widget()\n",
    "sort_button.on_click(on_sort)\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='Search for segments',\n",
    "    disabled=False,\n",
    "    button_style='danger'\n",
    ")\n",
    "def on_search(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        global TOPIC_SEGMENTS\n",
    "        TOPIC_SEGMENTS = find_segments(ANCHOR_WORDS, CONTEXT_WORDS)\n",
    "search_button.on_click(on_search)\n",
    "        \n",
    "display(anchor_widget)\n",
    "display(context_widget)\n",
    "display(widgets.HBox([sort_button, search_button]))\n",
    "display(status_output)\n",
    "sync_anchor_widget()\n",
    "sync_context_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Running `display_segments()` shows the retreived topic segments with a timeline. You must have hit 'search for segments' prior to running this.\n",
    "\n",
    "Timeline colors:\n",
    "- Red = topic segment\n",
    "- Blue = anchor phrases\n",
    "- Orange = context phrases\n",
    "- Grey = commercia\n",
    "\n",
    "Videos will be ordered by descending amount of time identified as the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T00:10:29.719674Z",
     "start_time": "2019-02-09T00:10:29.562817Z"
    },
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def show_filter_widgets():\n",
    "    channel_filter_button = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'CNN', 'FOXNEWS', 'MSNBC'],\n",
    "        value='All',\n",
    "        description='Channel:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    canonical_show_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All'] + list(sorted(MAJOR_CANONICAL_SHOWS)),\n",
    "        value='All',\n",
    "        description='Show:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    start_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Start date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    end_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='End date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global FILTER_WIDGETS\n",
    "    FILTER_WIDGETS = {\n",
    "        'show': canonical_show_dropdown,\n",
    "        'channel': channel_filter_button,\n",
    "        'start_date': start_date_picker,\n",
    "        'end_date': end_date_picker\n",
    "    }\n",
    "    display(widgets.HBox([\n",
    "        channel_filter_button, canonical_show_dropdown, \n",
    "        start_date_picker, end_date_picker]))\n",
    "    \n",
    "def get_filters():\n",
    "    filters = {}\n",
    "    show = FILTER_WIDGETS['show'].value\n",
    "    if show != 'All':\n",
    "        filters['show'] = show\n",
    "    channel = FILTER_WIDGETS['channel'].value\n",
    "    if channel != 'All':\n",
    "        filters['channel'] = channel\n",
    "    if FILTER_WIDGETS['start_date'].value:\n",
    "        filters['start'] = FILTER_WIDGETS['start_date'].value\n",
    "    if FILTER_WIDGETS['end_date'].value:\n",
    "        filters['end'] = FILTER_WIDGETS['end_date'].value \n",
    "    return filters\n",
    "\n",
    "def show_video_controls():\n",
    "    show_videos_output = widgets.Output()\n",
    "    limit_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=1000,\n",
    "        min=1,\n",
    "        max=10000,\n",
    "        description='Video limit:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    results_per_page_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=25,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        description='Results per page:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    show_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Show videos',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    def on_show_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "            display_segments(\n",
    "                TOPIC_SEGMENTS, get_filters(), \n",
    "                limit=limit_slider.value,\n",
    "                results_per_page=results_per_page_slider.value)\n",
    "    show_videos_button.on_click(on_show_videos)\n",
    "    clear_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Dismiss videos',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_clear_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "    clear_videos_button.on_click(on_clear_videos)\n",
    "    display(widgets.HBox([\n",
    "        show_videos_button, clear_videos_button,\n",
    "        limit_slider, results_per_page_slider]))\n",
    "    display(show_videos_output)\n",
    "\n",
    "show_filter_widgets()\n",
    "show_video_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Once we have some segments corresponding to the lexicon, we can use NLP to propose new context words to improve story coverage. `propose_context_words()` will use statistics to suggest new lexicon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T21:34:00.980789Z",
     "start_time": "2019-02-08T21:33:57.760601Z"
    }
   },
   "outputs": [],
   "source": [
    "propose_context_words(TOPIC_SEGMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Statistics\n",
    "\n",
    "`analysis()` will compute statistics over the story segments retreived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T00:08:46.377217Z",
     "start_time": "2019-02-09T00:08:38.682494Z"
    }
   },
   "outputs": [],
   "source": [
    "analysis(TOPIC_SEGMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a debugging lexicon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T00:08:26.697958Z",
     "start_time": "2019-02-09T00:08:22.195451Z"
    }
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORDS = {\n",
    "    'HURRICANE IRMA', 'IRMA'\n",
    "}\n",
    "CONTEXT_WORDS = { \n",
    "    'ADVISORY', 'ATLANTIC', 'BANDS', 'BEACH', 'BOATS', 'BRACING', 'BRIDGES',\n",
    "    'CARIBBEAN', 'CATASTROPHIC', 'CATEGORY', 'CLEANUP', 'COAST', 'COASTAL',\n",
    "    'CUBA', 'DAMAGE', 'DEBRIS', 'DESTRUCTION', 'DESTRUCTIVE', 'DEVASTATED',\n",
    "    'DEVASTATING', 'DEVASTATION', 'DISASTERS', 'DOWNTOWN', 'ELECTRICITY',\n",
    "    'EVACUATE', 'EVACUATED', 'EVACUATION', 'EVACUATIONS', 'FEMA', 'FLOOD',\n",
    "    'FLOODED', 'FLOODING', 'FLORIDA', 'FORECAST', 'GUSTS', 'HARVEY', 'HURRICANE',\n",
    "    'HURRICANES', 'IMPACTED', 'IMPACTS', 'INTENSITY', 'IRMA', 'ISLAND', 'ISLANDS',\n",
    "    'JOSE', 'KEYS', 'LANDFALL', 'MANDATORY', 'METEOROLOGIST', 'MIAMI', 'MONSTER',\n",
    "    'MYERS', 'NURSING', 'ORLANDO', 'OUTAGES', 'OUTER', 'PALM', 'POWER',\n",
    "    'PREPARATION', 'PUERTO', 'RAIN', 'RAINFALL', 'RAINS', 'REBUILD',\n",
    "    'RESPONDERS', 'RESTORED', 'RICO', 'SHELTER', 'SHELTERS', 'STORM',\n",
    "    'STORMS', 'STRONGEST', 'SUPPLIES', 'SURGE', 'SUSTAINED', 'TAMPA',\n",
    "    'TIDE', 'TREES', 'TROPICAL', 'WARNINGS', 'WATER', 'WAVES', 'WIND', 'WINDS'\n",
    "}\n",
    "sync_context_widget()\n",
    "sync_anchor_widget()\n",
    "on_search(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
