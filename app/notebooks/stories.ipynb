{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:46:03.799009Z",
     "start_time": "2019-02-14T07:46:01.518218Z"
    },
    "code_folding": [],
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter, namedtuple\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Initializing notebook. Please wait...', file=sys.stderr)\n",
    "\n",
    "import esper.captions as captions\n",
    "from captions.util import PostingUtil\n",
    "from esper.major_canonical_shows import MAJOR_CANONICAL_SHOWS\n",
    "from esper.widget import *\n",
    "from esper.rekall import *\n",
    "from rekall.interval_list import IntervalList\n",
    "\n",
    "WIDGET_STYLE_ARGS = {'description_width': 'initial'}\n",
    "\n",
    "ANCHOR_WORD_WINDOW_SIZE = 15\n",
    "CONTEXT_WORD_EXTEND_THRESH = 120\n",
    "\n",
    "GroundTruth = namedtuple('GroundTruth', ['true', 'false', 'intervals'])\n",
    "        \n",
    "\n",
    "def extend_postings(postings, threshold):\n",
    "    merged = []\n",
    "    curr_p = None\n",
    "    for p in postings:\n",
    "        if curr_p is None:\n",
    "            curr_p = p\n",
    "        elif p.start >= curr_p.start and p.start - curr_p.end <= threshold:\n",
    "            curr_p = PostingUtil.merge(curr_p, p)\n",
    "        else:\n",
    "            merged.append(curr_p)\n",
    "            curr_p = p\n",
    "    else:\n",
    "        merged.append(curr_p)\n",
    "    return merged\n",
    "\n",
    "def extend_postings_with_context(anchors, contexts, threshold):\n",
    "    results = []\n",
    "    for anchor_p in anchors:\n",
    "        for context_p in contexts:\n",
    "            if context_p.start >= anchor_p.start and context_p.start - anchor_p.end <= threshold:\n",
    "                anchor_p = PostingUtil.merge(anchor_p, context_p)\n",
    "        for context_p in contexts[::-1]:\n",
    "            if context_p.start <= anchor_p.start and anchor_p.start - context_p.end <= threshold:\n",
    "                anchor_p = PostingUtil.merge(anchor_p, context_p)\n",
    "        results.append(anchor_p)\n",
    "    return extend_postings(results, threshold)\n",
    "\n",
    "def filter_dict(d, keys):\n",
    "    return {k: v for k, v in d.items() if k in keys}\n",
    "\n",
    "TopicSegments = namedtuple('TopicSegments', [\n",
    "    'video_to_segments', 'video_to_anchor_words', 'video_to_context_words'\n",
    "])\n",
    "\n",
    "def or_queries(queries):\n",
    "    query = '|'.join('({})'.format(q) for q in queries)\n",
    "    return query\n",
    "\n",
    "def find_segments(anchor_words, context_words):\n",
    "    print('Searching for segments...'.format(len(anchor_words), len(context_words)), \n",
    "          file=sys.stderr)\n",
    "    \n",
    "    # Find the anchor locations\n",
    "    video_anchor_locations = {}\n",
    "    for d in captions.query_search(or_queries(anchor_words)):\n",
    "        doc = captions.get_document(d.id)\n",
    "        doc_duration = captions.INDEX.document_duration(doc)\n",
    "        video_anchor_locations[d.id] = PostingUtil.dilate(\n",
    "            d.postings, ANCHOR_WORD_WINDOW_SIZE, doc_duration)\n",
    "    \n",
    "    # Search for context locations\n",
    "    video_context_locations = {}\n",
    "    for d in captions.query_search(or_queries(context_words), \n",
    "                                   video_ids=video_anchor_locations.keys()):\n",
    "        video_context_locations[d.id] = list(d.postings)\n",
    "    \n",
    "    # Extend the anchor locations\n",
    "    video_topic_segments = {}\n",
    "    for video_id, anchor_postings in video_anchor_locations.items():\n",
    "        story_segments = extend_postings_with_context(\n",
    "            anchor_postings, video_context_locations.get(video_id, []),\n",
    "            CONTEXT_WORD_EXTEND_THRESH)\n",
    "        video_topic_segments[video_id] = story_segments\n",
    "    \n",
    "    coverage_seconds = sum(sum(p.end - p.start for p in l) for l in video_topic_segments.values())\n",
    "    print('Found {} segments in {} videos covering {:0.2f} minutes.'.format(\n",
    "        sum(len(l) for l in video_topic_segments.values()),\n",
    "        len(video_topic_segments),\n",
    "        coverage_seconds / 60\n",
    "    ), file=sys.stderr)\n",
    "    return TopicSegments(\n",
    "        video_topic_segments, \n",
    "        filter_dict(video_anchor_locations, video_topic_segments), \n",
    "        filter_dict(video_context_locations, video_topic_segments))\n",
    "\n",
    "MIN_TOKEN_COUNT = 10000\n",
    "\n",
    "def propose_context_words(topic_result, k=192, ncols=8, default_threshold=3.):\n",
    "    topic_word_counts = Counter()\n",
    "    for video_id, segments in topic_result.video_to_segments.items():\n",
    "        d = captions.get_document(video_id)\n",
    "        for p in segments:\n",
    "            topic_word_counts.update(captions.INDEX.tokens(d, p.idx, p.len))\n",
    "\n",
    "    all_words_total = sum(w.count for w in captions.LEXICON)\n",
    "    topic_words_total = sum(topic_word_counts.values())\n",
    "    \n",
    "    def filter_cond(t):\n",
    "        if t not in captions.LEXICON: \n",
    "            return False\n",
    "        w = captions.LEXICON[t]\n",
    "        return w.count > MIN_TOKEN_COUNT and w.token not in CONTEXT_WORDS\n",
    "\n",
    "    const_expr = math.log(all_words_total) - math.log(topic_words_total) \n",
    "    log_pmis = [\n",
    "        (t, math.log(topic_word_counts[t]) - math.log(captions.LEXICON[t].count) + const_expr)\n",
    "        for t in topic_word_counts.keys() if filter_cond(t)\n",
    "    ]\n",
    "    log_pmis.sort(key=lambda x: -x[1])\n",
    "    log_pmis = log_pmis[:k]\n",
    "    \n",
    "    selections = []\n",
    "    for t, score in log_pmis:\n",
    "        token = captions.LEXICON[t].token\n",
    "        w = widgets.ToggleButton(\n",
    "            value=score >= default_threshold,\n",
    "            description=token,\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "            icon=''\n",
    "        )\n",
    "        selections.append((t, w))\n",
    "    \n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    def on_submit(b):\n",
    "        selected_words = []\n",
    "        for t, w in selections:\n",
    "            if w.value == True:\n",
    "                selected_words.append(captions.LEXICON[t].token)\n",
    "        clear_output()\n",
    "        print('Added {} words to the context.'.format(len(selected_words)))\n",
    "        \n",
    "        global CONTEXT_WORDS\n",
    "        CONTEXT_WORDS.update(selected_words)\n",
    "        sync_context_widget()\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    hboxes = []\n",
    "    for i in range(0, len(selections), ncols):\n",
    "        hboxes.append(widgets.HBox([w for _, w in selections[i:i + ncols]]))\n",
    "    vbox = widgets.VBox(hboxes)\n",
    "    display(widgets.HBox([\n",
    "        widgets.Label(\n",
    "            'Instructions: Select new context words and hit submit. '\n",
    "            '(Likely words may already be highlighted.) '),\n",
    "        submit_button\n",
    "    ]))\n",
    "    display(vbox)\n",
    "    \n",
    "def display_segments(topic_results, filters={}, limit=1000, results_per_page=50, exclude=set()):\n",
    "    video_to_topic_time = {\n",
    "        video_id : sum(p.end - p.start for p in postings)\n",
    "        for video_id, postings in topic_results.video_to_segments.items()\n",
    "        if video_id not in exclude\n",
    "    }\n",
    "    video_qs = Video.objects.filter(id__in=list(video_to_topic_time.keys()), duplicate=False)\n",
    "    if 'show' in filters:\n",
    "        video_qs = video_qs.filter(show__canonical_show__name=filters['show'])\n",
    "    if 'channel' in filters:\n",
    "        video_qs = video_qs.filter(channel__name=filters['channel'])\n",
    "    if 'start' in filters:\n",
    "        video_qs = video_qs.filter(time__gte=filters['start'])\n",
    "    if 'end' in filters:\n",
    "        video_qs = video_qs.filter(time__lte=filters['end'])\n",
    "    video_to_fps = {\n",
    "        v['id']: v['fps'] for v in video_qs.values('id', 'fps', 'channel__name')\n",
    "    }\n",
    "    if len(video_to_fps) == 0:\n",
    "        print('No videos to display', file=sys.stderr)\n",
    "        return\n",
    "    video_to_topic_time = {k: v for k, v in video_to_topic_time.items() if k in video_to_fps}\n",
    "    limit_video_ids = set(sorted(video_to_fps, key=lambda x: -video_to_topic_time[x])[:limit])\n",
    "    \n",
    "    def convert_time(v, t):\n",
    "        return int(t * video_to_fps[v])\n",
    "    def to_intervallist(video_to_postings):\n",
    "        return {\n",
    "            video_id : IntervalList([\n",
    "                (convert_time(video_id, p.start), convert_time(video_id, p.end), None)\n",
    "                for p in postings\n",
    "            ]) \n",
    "            for video_id, postings in video_to_postings.items() \n",
    "            if video_id in limit_video_ids\n",
    "        }\n",
    "    \n",
    "    # Plot distribution of topic times in videos\n",
    "    def plot_dist_of_videos(results_per_page):\n",
    "        plt.figure(figsize=(7,2))\n",
    "        x = np.arange(len(video_to_topic_time))\n",
    "        y = np.array(sorted(video_to_topic_time.values(), key=lambda x: -x)) / 60\n",
    "        plt.plot(x, y, color='red')\n",
    "        plt.fill_betweenx([0, np.max(y)], len(limit_video_ids), alpha=0.2, color='gray')\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.ylim(0, np.max(y))\n",
    "        plt.xlabel('Video Number')\n",
    "        plt.xlim(0, len(video_to_topic_time))\n",
    "        plt.show()\n",
    "\n",
    "    print('Videos (ordered by descending segment time)')\n",
    "    plot_dist_of_videos(results_per_page)\n",
    "    print('Loading {} of {} videos (shaded region)... Please wait.'.format(\n",
    "        len(limit_video_ids), len(video_to_topic_time)))\n",
    "    \n",
    "    # Convert to intervallists\n",
    "    video_to_topic_intervals = to_intervallist(topic_results.video_to_segments)\n",
    "    video_to_anchor_intervals = to_intervallist(topic_results.video_to_anchor_words)\n",
    "    video_to_context_intervals = to_intervallist({\n",
    "        k: extend_postings(v, CONTEXT_WORD_EXTEND_THRESH) \n",
    "        # Coalesce context words to reduce memory usage\n",
    "        for k, v in topic_results.video_to_context_words.items()\n",
    "    })\n",
    "    video_to_commerical_intervals = qs_to_intrvllists(\n",
    "        Commercial.objects.filter(labeler__name='haotian-commercials',\n",
    "                                  video__id__in=limit_video_ids))\n",
    "    \n",
    "    # Display results\n",
    "    video_order = list(sorted(video_to_topic_intervals,\n",
    "        key=lambda x: -video_to_topic_time[x]))\n",
    "    result = intrvllists_to_result(\n",
    "        video_to_topic_intervals, color='red',\n",
    "        video_order=video_order)\n",
    "    add_intrvllists_to_result(result, video_to_anchor_intervals, color='blue')\n",
    "    add_intrvllists_to_result(result, video_to_context_intervals, color='orange')\n",
    "    add_intrvllists_to_result(result, video_to_commerical_intervals, color='black')\n",
    "    \n",
    "    video_widget = esper_widget(result, jupyter_keybindings=True, results_per_page=results_per_page)\n",
    "    update_output = widgets.Output()\n",
    "    update_button = widgets.Button(\n",
    "        description='Update ground truth',\n",
    "        disabled=False,\n",
    "        button_style='warning'\n",
    "    )\n",
    "    def on_update(b):\n",
    "        global GROUND_TRUTH\n",
    "        selected_ids = [video_order[i] for i in video_widget.selected]\n",
    "        ignored_ids = [video_order[i] for i in video_widget.ignored]\n",
    "        GROUND_TRUTH.true.update(selected_ids)\n",
    "        GROUND_TRUTH.false.update(ignored_ids)\n",
    "        GROUND_TRUTH.true.difference_update(ignored_ids)\n",
    "        GROUND_TRUTH.false.difference_update(selected_ids)\n",
    "        for i in selected_ids:\n",
    "            GROUND_TRUTH.intervals[i] = video_to_topic_intervals[i]\n",
    "        for i in ignored_id:\n",
    "            GROUND_TRUTH.intervals[i] = video_to_topic_intervals[i]\n",
    "        with update_output:\n",
    "            if len(selected_ids) + len(ignored_ids) > 0:\n",
    "                print('Added {} true and {} false labels'.format(\n",
    "                    len(selected_ids), len(ignored_ids)))\n",
    "            print('Ground truth: true={}, false={}'.format(\n",
    "                len(GROUND_TRUTH.true), len(GROUND_TRUTH.false)))\n",
    "                \n",
    "    update_button.on_click(on_update)\n",
    "\n",
    "    with update_output:\n",
    "        print('Select postive labels with [ and negative labels with ].')\n",
    "    display(widgets.HBox([update_button, update_output]))\n",
    "    display(video_widget)\n",
    "    \n",
    "def is_uniformly_sampled(is_3y, fps, frame_number):\n",
    "    if is_3y:\n",
    "        return frame_number % math.floor(fps * 3) == 0\n",
    "    else:\n",
    "        return frame_number % math.ceil(fps * 3) == 0\n",
    "\n",
    "try:\n",
    "    _FACE_IDENTS\n",
    "except NameError:\n",
    "    _FACE_IDENTS = None\n",
    "def get_face_idents():\n",
    "    global _FACE_IDENTS\n",
    "    if _FACE_IDENTS is None:\n",
    "        print('Loading face identities...', file=sys.stderr)\n",
    "        with open('/app/data/identities_by_video.pkl', 'rb') as f:\n",
    "            _FACE_IDENTS = pickle.load(f)\n",
    "    else:\n",
    "        print('Face identities already loaded.', file=sys.stderr)\n",
    "    return _FACE_IDENTS\n",
    "\n",
    "try:\n",
    "    _FACE_GENDERS\n",
    "except NameError:\n",
    "    _FACE_GENDERS = None\n",
    "def get_face_genders():\n",
    "    global _FACE_GENDERS\n",
    "    if _FACE_GENDERS is None:\n",
    "        print('Loading face genders...', file=sys.stderr)\n",
    "        with open('/app/data/face_genders_by_video.pkl', 'rb') as f:\n",
    "            _FACE_GENDERS = pickle.load(f)\n",
    "    else:\n",
    "        print('Face genders already loaded.', file=sys.stderr)\n",
    "    return _FACE_GENDERS\n",
    "    \n",
    "def analysis(topic_results, n=10000):\n",
    "    video_to_meta = {\n",
    "        v['id']: {\n",
    "            'channel': v['channel__name'],\n",
    "            'show': v['show__canonical_show__name'],\n",
    "            'time': v['time'],\n",
    "            'fps': v['fps'],\n",
    "            'is_3y': v['threeyears_dataset'],\n",
    "            'path': v['path']\n",
    "        } for v in Video.objects.filter(\n",
    "            id__in=list(topic_results.video_to_segments.keys()), \n",
    "            duplicate=False\n",
    "        ).values(\n",
    "            'id', 'channel__name', 'show__canonical_show__name', 'time', 'fps',\n",
    "            'threeyears_dataset', 'path'\n",
    "        )\n",
    "    }\n",
    "    channels = [c.name for c in Channel.objects.all()]\n",
    "    utc = timezone('UTC')\n",
    "    eastern = timezone('US/Eastern')\n",
    "    \n",
    "    channel_to_time = {c: 0. for c in channels}\n",
    "    channel_to_daypart_to_time = {c: np.zeros(24) for c in channels}\n",
    "    channel_to_weekday_to_time = {c: np.zeros(7) for c in channels}\n",
    "    channel_to_time_to_time = {c: defaultdict(float) for c in channels}\n",
    "    show_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "\n",
    "        video_topic_len = sum(p.end - p.start for p in postings)\n",
    "        channel = video_to_meta[video_id]['channel']\n",
    "        channel_to_time[channel] += video_topic_len\n",
    "        \n",
    "        video_dt = utc.localize(video_to_meta[video_id]['time']).astimezone(eastern)\n",
    "        for p in postings:\n",
    "            base_hour = video_dt.hour\n",
    "            posting_len = p.end - p.start\n",
    "            channel_to_daypart_to_time[channel][\n",
    "                (base_hour + int(p.start / 3600)) % 24\n",
    "            ] += posting_len\n",
    "            \n",
    "        channel_to_weekday_to_time[channel][video_dt.weekday()] += video_topic_len\n",
    "        channel_to_time_to_time[channel][video_dt.date()] += video_topic_len\n",
    "        \n",
    "        show = video_to_meta[video_id]['show']\n",
    "        show_to_time[(channel, show)] += video_topic_len\n",
    "        \n",
    "    print('Topic time by channel:')\n",
    "    for c in channel_to_time:\n",
    "        print('  {}: {:0.3f} hours'.format(c, channel_to_time[c] / 3600))\n",
    "        \n",
    "    print('\\nTopic time by daypart:')\n",
    "    def plot_daypart():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(24) + (i - 1) * bar_width,\n",
    "                    channel_to_daypart_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(24))\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.show()\n",
    "    plot_daypart()\n",
    "    \n",
    "    print('\\nTopic time by weekday:')\n",
    "    def plot_weekday():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(7) + (i - 1) * bar_width, \n",
    "                    channel_to_weekday_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.show()\n",
    "    plot_weekday()\n",
    "    \n",
    "    print('\\nTopic time by day:')\n",
    "    def plot_timeline():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for c in channels:\n",
    "            data = [x for x in sorted(channel_to_time_to_time[c].items())]\n",
    "            plt.scatter(\n",
    "                [x for x, _ in data], [y / 60 for _, y in data],\n",
    "                alpha=0.5, s=2, label=c)\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Day')\n",
    "        plt.show()\n",
    "    plot_timeline()\n",
    "    \n",
    "    top_n = 10\n",
    "    print('\\nShows with most coverage (top-{}):'.format(top_n))\n",
    "    for (channel, show), seconds in show_to_time.most_common(top_n):\n",
    "        print('  {} ({}): {:0.1f} minutes'.format(show, channel, seconds / 60))\n",
    "\n",
    "    def join_face_labels_and_postings(labels, postings, label_len=3):\n",
    "        result = Counter()\n",
    "        try:\n",
    "            label_head = next(labels)\n",
    "            postings_head = next(postings)\n",
    "            while True:\n",
    "                if label_head[1] > postings_head.end:\n",
    "                    postings_head = next(postings)\n",
    "                elif label_head[1] + label_len < postings_head.start:\n",
    "                    label_head = next(labels)\n",
    "                else:\n",
    "                    result[label_head[0]] += label_len\n",
    "                    label_head = next(labels)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        return result\n",
    "    # Time by Gender\n",
    "    face_genders = get_face_genders()\n",
    "    gender_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        video_genders = face_genders.get(video_id, [])\n",
    "        gender_to_time.update(join_face_labels_and_postings(iter(video_genders), iter(postings)))\n",
    "    male_prop = gender_to_time[1] / sum(gender_to_time[k] for k in gender_to_time)\n",
    "    print('\\nGender screen time: male={:0.2f} female={:0.2f}'.format(male_prop, 1 - male_prop))\n",
    "\n",
    "#     video_to_face_genders = defaultdict(list)\n",
    "#     face_gender_qs = FaceGender.objects.filter(\n",
    "#         labeler=Labeler.objects.get(name='knn-gender'), \n",
    "#         face__frame__video__id__in=list(video_to_meta.keys()),\n",
    "#     )\n",
    "#     for f in face_gender_qs.values(\n",
    "#         'gender__name', 'face__frame__video__id', 'face__frame__number', 'probability'\n",
    "#     ).order_by('?')[:n]:\n",
    "#         video_id = f['face__frame__video__id']\n",
    "#         fps = video_to_meta[video_id]['fps']\n",
    "#         is_3y = video_to_meta[video_id]['is_3y']\n",
    "#         frame_number = f['face__frame__number']\n",
    "#         if not is_uniformly_sampled(is_3y, fps, frame_number):\n",
    "#             continue\n",
    "#         start = frame_number / fps\n",
    "#         video_to_face_genders[video_id].append({\n",
    "#             'label': f['gender__name'],\n",
    "#             'probability': f['probability'],\n",
    "#             'start': start,\n",
    "#             'end': start + 3\n",
    "#         })\n",
    "        \n",
    "    # Time by Identity\n",
    "    face_idents = get_face_idents()\n",
    "    ident_id_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        video_idents = face_idents.get(video_id, [])\n",
    "        ident_id_to_time.update(join_face_labels_and_postings(iter(video_idents), iter(postings)))\n",
    "    top_n = 10\n",
    "    print('\\nPeople with most screen time (top-{}):'.format(top_n))\n",
    "    for ident_id, seconds in ident_id_to_time.most_common(top_n):\n",
    "        print('  {}: {:0.1f} minutes'.format(\n",
    "            Identity.objects.get(id=ident_id).name, \n",
    "            seconds / 60))\n",
    "        \n",
    "#     print('Not implemented...', file=sys.stderr)\n",
    "#     ident_labelers = [\n",
    "#         l['id'] for l in Labeler.objects.filter(\n",
    "#             name__contains='face-identity'\n",
    "#         ).exclude(name__contains='face-identity-old:').values('id')\n",
    "#     ]\n",
    "#     video_to_face_idents = defaultdict(list)\n",
    "#     face_ident_qs = FaceIdentity.objects.filter(\n",
    "#         labeler__id__in=ident_labelers, \n",
    "#         face__frame__video__id__in=list(video_to_meta.keys()),\n",
    "#         probability__gt=0.5\n",
    "#     )\n",
    "#     for f in face_ident_qs.values(\n",
    "#         'identity__name', 'face__frame__video__id', 'face__frame__number', 'probability'\n",
    "#     ).order_by('?')[:n]:\n",
    "#         video_id = f['face__frame__video__id']\n",
    "#         fps = video_to_meta[video_id]['fps']\n",
    "#         is_3y = video_to_meta[video_id]['is_3y']\n",
    "#         frame_number = f['frame__number']\n",
    "#         if not is_uniformly_sampled(is_3y, fps, frame_number):\n",
    "#             continue\n",
    "#         start = frame_number / fps\n",
    "#         video_to_face_idents[video_id].append({\n",
    "#             'label': f['identity__name'],\n",
    "#             'probability': f['probability'],\n",
    "#             'start': start,\n",
    "#             'end': start + 3\n",
    "#         })\n",
    "\n",
    "print('Done initializing notebook.', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Stories from a Lexicon\n",
    "\n",
    "Stories are retreived via lexicons of words. Story lexicons have two components, <b>anchor phrases</b> and <b>context phrases</b>.\n",
    "\n",
    "<b>Anchor phrases</b> are phrases that must appear for a segment to be considered a part of a story and should be unique to the story. For instance, a segment about 'Hurricane Irma' must mention either 'Irma' or 'Hurricane Irma'.\n",
    "\n",
    "<b>Context phrases</b> are phrases that are used to describe the story and are used to determine that time extent of a topic segment. For instance, words such as 'devastation' and 'storm' around an anchor such as 'Hurricane Irma' indicate that the discussion is part of the story. These phrases need not be unique to the story.\n",
    "\n",
    "<b>Instructions:</b>\n",
    "- Enter anchor phrases to start (required; see caption-index query syntax)\n",
    "- Enter a few context phrases (optional)\n",
    "- Hit 'search for segments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:46:03.934329Z",
     "start_time": "2019-02-14T07:46:03.801694Z"
    },
    "hide_input": true,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ANCHOR_WORDS\n",
    "    CONTEXT_WORDS\n",
    "except NameError:\n",
    "    ANCHOR_WORDS = set()\n",
    "    CONTEXT_WORDS = set()\n",
    "    \n",
    "try:\n",
    "    TOPIC_SEGMENTS\n",
    "    GROUND_TRUTH\n",
    "except NameError:\n",
    "    TOPIC_SEGMENTS = None\n",
    "    GROUND_TRUTH = GroundTruth(set(), set(), {})\n",
    "\n",
    "status_output = widgets.Output()\n",
    "anchor_widget = widgets.Textarea(\n",
    "    style=WIDGET_STYLE_ARGS,\n",
    "    value='',\n",
    "    layout=widgets.Layout(width='100%'),\n",
    "    placeholder='Phrases (one per line)',\n",
    "    description='Anchor phrases:',\n",
    "    disabled=False\n",
    ")\n",
    "def sync_anchor_widget():\n",
    "    anchor_widget.value = '\\n'.join(sorted(ANCHOR_WORDS))\n",
    "    anchor_widget.layout = widgets.Layout(\n",
    "        width='100%', \n",
    "        height='{}px'.format(20 * (len(ANCHOR_WORDS) + 2))\n",
    "    )\n",
    "def on_anchor_changed(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        try:\n",
    "            global ANCHOR_WORDS\n",
    "            ANCHOR_WORDS = {t.strip() for t in anchor_widget.value.split('\\n') if len(t.strip()) > 0}\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "anchor_widget.observe(on_anchor_changed, names='value')\n",
    "\n",
    "context_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    style=WIDGET_STYLE_ARGS,\n",
    "    layout=widgets.Layout(width='100%'),\n",
    "    placeholder='Phrases (one per line)',\n",
    "    description='Context phrases:',\n",
    "    disabled=False\n",
    ")\n",
    "def sync_context_widget():\n",
    "    context_widget.value = '\\n'.join(sorted(CONTEXT_WORDS))\n",
    "    context_widget.layout = widgets.Layout(\n",
    "        width='100%', \n",
    "        height='{}px'.format(20 * (len(CONTEXT_WORDS) + 2))\n",
    "    )\n",
    "def on_context_changed(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        try:\n",
    "            global CONTEXT_WORDS\n",
    "            CONTEXT_WORDS = {t.strip() for t in context_widget.value.split('\\n') if len(t.strip()) > 0}\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "context_widget.observe(on_context_changed, names='value')\n",
    "\n",
    "sort_button = widgets.Button(\n",
    "    description='Sort phrases',\n",
    "    disabled=False,\n",
    "    button_style=''\n",
    ")\n",
    "def on_sort(b):\n",
    "    sync_anchor_widget()\n",
    "    sync_context_widget()\n",
    "sort_button.on_click(on_sort)\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='Search for segments',\n",
    "    disabled=False,\n",
    "    button_style='danger'\n",
    ")\n",
    "def on_search(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        global TOPIC_SEGMENTS\n",
    "        TOPIC_SEGMENTS = find_segments(ANCHOR_WORDS, CONTEXT_WORDS)\n",
    "        print('Recall on ground truth: true={}/{} and false={}/{}'.format(\n",
    "            len(GROUND_TRUTH.true & set(TOPIC_SEGMENTS.video_to_segments.keys())), len(GROUND_TRUTH.true), \n",
    "            len(GROUND_TRUTH.false & set(TOPIC_SEGMENTS.video_to_segments.keys())), len(GROUND_TRUTH.false)\n",
    "        ), file=sys.stderr)\n",
    "search_button.on_click(on_search)\n",
    "\n",
    "display(anchor_widget)\n",
    "display(context_widget)\n",
    "display(widgets.HBox([sort_button, search_button]))\n",
    "display(status_output)\n",
    "sync_anchor_widget()\n",
    "sync_context_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Running `display_segments()` shows the retreived topic segments with a timeline. You must have hit 'search for segments' prior to running this.\n",
    "\n",
    "Timeline colors:\n",
    "- Red = topic segment\n",
    "- Blue = anchor phrases\n",
    "- Orange = context phrases\n",
    "- Grey = commercial\n",
    "\n",
    "Videos will be ordered by descending amount of time identified as the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:46:04.100251Z",
     "start_time": "2019-02-14T07:46:03.936609Z"
    },
    "hide_input": true,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_filter_widgets():\n",
    "    channel_filter_button = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'CNN', 'FOXNEWS', 'MSNBC'],\n",
    "        value='All',\n",
    "        description='Channel:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    canonical_show_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All'] + list(sorted(MAJOR_CANONICAL_SHOWS)),\n",
    "        value='All',\n",
    "        description='Show:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    start_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Start date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    end_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='End date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global FILTER_WIDGETS\n",
    "    FILTER_WIDGETS = {\n",
    "        'show': canonical_show_dropdown,\n",
    "        'channel': channel_filter_button,\n",
    "        'start_date': start_date_picker,\n",
    "        'end_date': end_date_picker\n",
    "    }\n",
    "    display(widgets.HBox([\n",
    "        channel_filter_button, canonical_show_dropdown, \n",
    "        start_date_picker, end_date_picker]))\n",
    "    \n",
    "def get_filters():\n",
    "    filters = {}\n",
    "    show = FILTER_WIDGETS['show'].value\n",
    "    if show != 'All':\n",
    "        filters['show'] = show\n",
    "    channel = FILTER_WIDGETS['channel'].value\n",
    "    if channel != 'All':\n",
    "        filters['channel'] = channel\n",
    "    if FILTER_WIDGETS['start_date'].value:\n",
    "        filters['start'] = FILTER_WIDGETS['start_date'].value\n",
    "    if FILTER_WIDGETS['end_date'].value:\n",
    "        filters['end'] = FILTER_WIDGETS['end_date'].value \n",
    "    return filters\n",
    "\n",
    "def show_video_controls():\n",
    "    show_videos_output = widgets.Output()\n",
    "    limit_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=1000,\n",
    "        min=1,\n",
    "        max=10000,\n",
    "        description='Video limit:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    results_per_page_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=25,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        description='Results per page:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    show_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Show videos',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    filter_videos_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'Labeled True', 'Labeled False'],\n",
    "        value='All',\n",
    "        description='Videos:',\n",
    "        disabled=False\n",
    "    )\n",
    "    def on_show_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "            if filter_videos_dropdown.value == 'All':\n",
    "                exclude = set()\n",
    "            elif filter_videos_dropdown.value == 'Labeled True':\n",
    "                exclude = {\n",
    "                    x for x in TOPIC_SEGMENTS.video_to_segments\n",
    "                    if x not in GROUND_TRUTH.true\n",
    "                }\n",
    "            else:\n",
    "                exclude = {\n",
    "                    x for x in TOPIC_SEGMENTS.video_to_segments \n",
    "                    if x not in GROUND_TRUTH.false\n",
    "                }\n",
    "            display_segments(\n",
    "                TOPIC_SEGMENTS, get_filters(),\n",
    "                limit=limit_slider.value,\n",
    "                results_per_page=results_per_page_slider.value,\n",
    "                exclude=exclude\n",
    "            )\n",
    "    show_videos_button.on_click(on_show_videos)\n",
    "    clear_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Dismiss videos',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_clear_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "    clear_videos_button.on_click(on_clear_videos)\n",
    "    display(widgets.HBox([\n",
    "        show_videos_button, clear_videos_button,\n",
    "        limit_slider, results_per_page_slider, filter_videos_dropdown]))\n",
    "    display(show_videos_output)\n",
    "\n",
    "show_filter_widgets()\n",
    "show_video_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Once we have some segments corresponding to the lexicon, we can use NLP to propose new context words to improve story coverage. `propose_context_words()` will use statistics to suggest new lexicon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T19:34:26.198194Z",
     "start_time": "2019-02-13T19:34:24.133707Z"
    }
   },
   "outputs": [],
   "source": [
    "propose_context_words(TOPIC_SEGMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Statistics\n",
    "\n",
    "`analysis()` will compute statistics over the story segments retreived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-14T01:58:01.798Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "analysis(TOPIC_SEGMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a debugging lexicon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:40:10.235736Z",
     "start_time": "2019-02-14T07:40:09.458085Z"
    }
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORDS = {\n",
    "    'HURRICANE IRMA', 'IRMA'\n",
    "}\n",
    "CONTEXT_WORDS = { \n",
    "    'ADVISORY', 'ATLANTIC', 'BANDS', 'BEACH', 'BOATS', 'BRACING', 'BRIDGES',\n",
    "    'CARIBBEAN', 'CATASTROPHIC', 'CATEGORY', 'CLEANUP', 'COAST', 'COASTAL',\n",
    "    'CUBA', 'DAMAGE', 'DEBRIS', 'DESTRUCTION', 'DESTRUCTIVE', 'DEVASTATED',\n",
    "    'DEVASTATING', 'DEVASTATION', 'DISASTERS', 'DOWNTOWN', 'ELECTRICITY',\n",
    "    'EVACUATE', 'EVACUATED', 'EVACUATION', 'EVACUATIONS', 'FEMA', 'FLOOD',\n",
    "    'FLOODED', 'FLOODING', 'FLORIDA', 'FORECAST', 'GUSTS', 'HARVEY', 'HURRICANE',\n",
    "    'HURRICANES', 'IMPACTED', 'IMPACTS', 'INTENSITY', 'IRMA', 'ISLAND', 'ISLANDS',\n",
    "    'JOSE', 'KEYS', 'LANDFALL', 'MANDATORY', 'METEOROLOGIST', 'MIAMI', 'MONSTER',\n",
    "    'MYERS', 'NURSING', 'ORLANDO', 'OUTAGES', 'OUTER', 'PALM', 'POWER',\n",
    "    'PREPARATION', 'PUERTO', 'RAIN', 'RAINFALL', 'RAINS', 'REBUILD',\n",
    "    'RESPONDERS', 'RESTORED', 'RICO', 'SHELTER', 'SHELTERS', 'STORM',\n",
    "    'STORMS', 'STRONGEST', 'SUPPLIES', 'SURGE', 'SUSTAINED', 'TAMPA',\n",
    "    'TIDE', 'TREES', 'TROPICAL', 'WARNINGS', 'WATER', 'WAVES', 'WIND', 'WINDS'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth(set(), set(), {})\n",
    "TOPIC_SEGMENTS = None\n",
    "sync_context_widget()\n",
    "sync_anchor_widget()\n",
    "on_search(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
