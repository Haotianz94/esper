{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T22:05:53.323Z"
    },
    "code_folding": [],
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter, namedtuple\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Initializing notebook. Please wait...', file=sys.stderr)\n",
    "\n",
    "import esper.captions as captions\n",
    "from captions.util import PostingUtil\n",
    "from esper.major_canonical_shows import MAJOR_CANONICAL_SHOWS\n",
    "from esper.widget import *\n",
    "from esper.rekall import *\n",
    "from rekall.interval_list import IntervalList\n",
    "from captions import CaptionIndex\n",
    "\n",
    "WIDGET_STYLE_ARGS = {'description_width': 'initial'}\n",
    "\n",
    "GroundTruth = namedtuple('GroundTruth', ['positive', 'negative'])\n",
    "        \n",
    "\n",
    "def extend_postings(postings, threshold):\n",
    "    # This does a merge with threshold\n",
    "    return PostingUtil.deoverlap(postings, threshold)\n",
    "\n",
    "def extend_postings_with_context(anchors, contexts, threshold):\n",
    "    results = []\n",
    "    for anchor_p in anchors:\n",
    "        for context_p in contexts:\n",
    "            if context_p.start >= anchor_p.start and context_p.start - anchor_p.end <= threshold:\n",
    "                anchor_p = PostingUtil.merge(anchor_p, context_p)\n",
    "        for context_p in contexts[::-1]:\n",
    "            if context_p.start <= anchor_p.start and anchor_p.start - context_p.end <= threshold:\n",
    "                anchor_p = PostingUtil.merge(anchor_p, context_p)\n",
    "        results.append(anchor_p)\n",
    "    return extend_postings(results, threshold)\n",
    "\n",
    "def filter_dict(d, keys):\n",
    "    return {k: v for k, v in d.items() if k in keys}\n",
    "\n",
    "TopicSegments = namedtuple('TopicSegments', [\n",
    "    'video_to_segments', 'video_to_anchor_words', 'video_to_context_words'\n",
    "])\n",
    "\n",
    "def or_queries(queries):\n",
    "    query = '|'.join('({})'.format(q) for q in queries)\n",
    "    return query\n",
    "\n",
    "def find_segments(anchor_words, context_words):\n",
    "    print('Searching for segments...'.format(len(anchor_words), len(context_words)), \n",
    "          file=sys.stderr)\n",
    "    \n",
    "    # Find the anchor locations\n",
    "    video_anchor_locations = {}\n",
    "    for d in captions.query_search(or_queries(anchor_words).upper()):\n",
    "        doc = captions.get_document(d.id)\n",
    "        doc_duration = captions.INDEX.document_duration(doc)\n",
    "        video_anchor_locations[d.id] = extend_postings(\n",
    "            PostingUtil.dilate(\n",
    "                d.postings, ANCHOR_WORD_WINDOW_SIZE, doc_duration), 0)\n",
    "    \n",
    "    # Search for context locations\n",
    "    video_context_locations = {}\n",
    "    if len(context_words) > 0:\n",
    "        for d in captions.query_search(or_queries(context_words).upper(), \n",
    "                                       video_ids=video_anchor_locations.keys()):\n",
    "            video_context_locations[d.id] = list(d.postings)\n",
    "    \n",
    "    # Extend the anchor locations\n",
    "    video_topic_segments = {}\n",
    "    for video_id, anchor_postings in video_anchor_locations.items():\n",
    "        story_segments = extend_postings_with_context(\n",
    "            anchor_postings, video_context_locations.get(video_id, []),\n",
    "            CONTEXT_WORD_EXTEND_THRESH)\n",
    "        story_segments = list(filter(\n",
    "            lambda p: p.end - p.start >= MIN_PROPOSED_SEGMENT_LEN,\n",
    "            story_segments))\n",
    "        video_topic_segments[video_id] = story_segments\n",
    "    \n",
    "    coverage_seconds = sum(sum(p.end - p.start for p in l) \n",
    "                           for l in video_topic_segments.values())\n",
    "    print('Found {} segments in {} videos covering {:0.2f} minutes.'.format(\n",
    "        sum(len(l) for l in video_topic_segments.values()),\n",
    "        len(video_topic_segments),\n",
    "        coverage_seconds / 60\n",
    "    ), file=sys.stderr)\n",
    "    return TopicSegments(\n",
    "        video_topic_segments,\n",
    "        filter_dict(video_anchor_locations, video_topic_segments), \n",
    "        filter_dict(video_context_locations, video_topic_segments))\n",
    "\n",
    "MIN_TOKEN_COUNT = 10000\n",
    "\n",
    "def propose_context_words(topic_result, k=192, ncols=8, default_threshold=3.):\n",
    "    topic_word_counts = Counter()\n",
    "    for video_id, segments in topic_result.video_to_segments.items():\n",
    "        d = captions.get_document(video_id)\n",
    "        for p in segments:\n",
    "            topic_word_counts.update(captions.INDEX.tokens(d, p.idx, p.len))\n",
    "\n",
    "    all_words_total = sum(w.count for w in captions.LEXICON)\n",
    "    topic_words_total = sum(topic_word_counts.values())\n",
    "    \n",
    "    def filter_cond(t):\n",
    "        if t not in captions.LEXICON: \n",
    "            return False\n",
    "        w = captions.LEXICON[t]\n",
    "        return w.count > MIN_TOKEN_COUNT and w.token not in CONTEXT_WORDS\n",
    "\n",
    "    const_expr = math.log(all_words_total) - math.log(topic_words_total) \n",
    "    log_pmis = [\n",
    "        (t, math.log(topic_word_counts[t]) - math.log(captions.LEXICON[t].count) + const_expr)\n",
    "        for t in topic_word_counts.keys() if filter_cond(t)\n",
    "    ]\n",
    "    log_pmis.sort(key=lambda x: -x[1])\n",
    "    log_pmis = log_pmis[:k]\n",
    "    \n",
    "    selections = []\n",
    "    for t, score in log_pmis:\n",
    "        token = captions.LEXICON[t].token\n",
    "        w = widgets.ToggleButton(\n",
    "            value=score >= default_threshold,\n",
    "            description=token,\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "            icon=''\n",
    "        )\n",
    "        selections.append((t, w))\n",
    "    \n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    def on_submit(b):\n",
    "        selected_words = []\n",
    "        for t, w in selections:\n",
    "            if w.value == True:\n",
    "                selected_words.append(captions.LEXICON[t].token)\n",
    "        clear_output()\n",
    "        print('Added {} words to the context.'.format(len(selected_words)))\n",
    "        \n",
    "        global CONTEXT_WORDS\n",
    "        CONTEXT_WORDS.update(selected_words)\n",
    "        sync_context_widget()\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    cancel_button = widgets.Button(\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_cancel(b):\n",
    "        clear_output()\n",
    "    cancel_button.on_click(on_cancel)\n",
    "    \n",
    "    hboxes = []\n",
    "    for i in range(0, len(selections), ncols):\n",
    "        hboxes.append(widgets.HBox([w for _, w in selections[i:i + ncols]]))\n",
    "    vbox = widgets.VBox(hboxes)\n",
    "    display(widgets.HBox([\n",
    "        widgets.Label(\n",
    "            'Instructions: Select new context words and hit submit. '\n",
    "            '(Likely words may already be highlighted.) '),\n",
    "        submit_button, cancel_button\n",
    "    ]))\n",
    "    display(vbox)\n",
    "    \n",
    "def filter_video_qs(video_qs, filters):\n",
    "    if 'show' in filters:\n",
    "        video_qs = video_qs.filter(show__canonical_show__name=filters['show'])\n",
    "    if 'channel' in filters:\n",
    "        video_qs = video_qs.filter(channel__name=filters['channel'])\n",
    "    if 'start' in filters:\n",
    "        video_qs = video_qs.filter(time__gte=filters['start'])\n",
    "    if 'end' in filters:\n",
    "        video_qs = video_qs.filter(time__lte=filters['end'])\n",
    "    return video_qs\n",
    "    \n",
    "def display_segments(topic_results, ground_truth,\n",
    "                     filters={}, limit=1000, results_per_page=50, \n",
    "                     exclude=set()):\n",
    "    video_to_topic_time = {\n",
    "        video_id : sum(p.end - p.start for p in postings)\n",
    "        for video_id, postings in topic_results.video_to_segments.items()\n",
    "        if video_id not in exclude\n",
    "    }\n",
    "    video_qs = Video.objects.filter(id__in=list(video_to_topic_time.keys()), \n",
    "                                    duplicate=False)\n",
    "    video_qs = filter_video_qs(video_qs, filters)\n",
    "    video_to_fps = {\n",
    "        v['id']: v['fps'] for v in video_qs.values('id', 'fps', 'channel__name')\n",
    "    }\n",
    "    if len(video_to_fps) == 0:\n",
    "        print('No videos to display', file=sys.stderr)\n",
    "        return\n",
    "    video_to_topic_time = {\n",
    "        k: v for k, v in video_to_topic_time.items() if k in video_to_fps}\n",
    "    limit_video_ids = set(sorted(video_to_fps, \n",
    "                                 key=lambda x: -video_to_topic_time[x])[:limit])\n",
    "    \n",
    "    def convert_time(v, t):\n",
    "        return int(t * video_to_fps[v])\n",
    "    def to_intervallist(video_to_postings):\n",
    "        return {\n",
    "            video_id : IntervalList([\n",
    "                (convert_time(video_id, p.start), convert_time(video_id, p.end), None)\n",
    "                for p in postings\n",
    "            ]) \n",
    "            for video_id, postings in video_to_postings.items() \n",
    "            if video_id in limit_video_ids\n",
    "        }\n",
    "    def compute_true_time(video_id):\n",
    "        intervals = ground_truth.positive.get(video_id, [])\n",
    "        return sum(b - a for a, b in intervals)\n",
    "    \n",
    "    # Plot distribution of topic times in videos\n",
    "    def plot_dist_of_videos(video_order):\n",
    "        fig, ax1 = plt.subplots(figsize=(7,2))\n",
    "        x = np.arange(len(video_to_topic_time))\n",
    "        y_pred = np.array([video_to_topic_time[v] for v in video_order]) / 60\n",
    "       \n",
    "        ax1.plot(x, y_pred, color='purple')\n",
    "        y_true_tmp = [compute_true_time(v) for v in video_order]\n",
    "        if sum(y_true_tmp) > 0:\n",
    "            x_true = np.array([i for i, y in enumerate(y_true_tmp) if y > 0])\n",
    "            y_true = np.array([y for y in y_true_tmp if y > 0]) / 60\n",
    "            ax1.plot(x_true, y_true, 'x', color='blue')\n",
    "            y_max = max(np.max(y_pred), np.max(y_true))\n",
    "        else:\n",
    "            y_max = np.max(y_pred)\n",
    "        ax1.fill_betweenx([0, y_max], len(limit_video_ids), alpha=0.2, color='gray')\n",
    "        ax1.set_ylabel('Minutes', color='purple')\n",
    "        ax1.tick_params('y', colors='purple')\n",
    "        ax1.set_ylim(0, y_max)\n",
    "        ax1.set_xlabel('Video Number')\n",
    "        ax1.set_xlim(0, len(video_to_topic_time))\n",
    "        y_prop = np.cumsum(y_pred)\n",
    "        y_prop *= 100. / y_prop[-1]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(x, y_prop, color='black')\n",
    "        ax2.set_ylabel('Cumulative % Minutes', color='black')\n",
    "        ax2.tick_params('y', colors='black')\n",
    "        plt.show()\n",
    "    \n",
    "    print('Videos (ordered by descending segment time)')\n",
    "    video_ids = set(video_to_topic_time.keys())\n",
    "    video_order = list(sorted(video_ids, key=lambda x: -video_to_topic_time[x]))\n",
    "    plot_dist_of_videos(video_order)\n",
    "    print('Loading {} of {} videos (shaded region)... Please wait.'.format(\n",
    "        len(limit_video_ids), len(video_to_topic_time)))\n",
    "    \n",
    "    # Convert to intervallists\n",
    "    video_to_topic_intervals = to_intervallist(topic_results.video_to_segments)\n",
    "    video_to_anchor_intervals = to_intervallist(topic_results.video_to_anchor_words)\n",
    "    video_to_context_intervals = to_intervallist({\n",
    "        k: extend_postings(v, 15) \n",
    "        # Coalesce context words to reduce memory usage\n",
    "        for k, v in topic_results.video_to_context_words.items()\n",
    "    })\n",
    "    video_to_commerical_intervals = qs_to_intrvllists(\n",
    "        Commercial.objects.filter(labeler__name='haotian-commercials',\n",
    "                                  video__id__in=limit_video_ids))\n",
    "    \n",
    "    def ranges_to_intrvllist(v, ranges):\n",
    "        return IntervalList([\n",
    "            (convert_time(v, a), convert_time(v, b), None) \n",
    "            for a, b in ranges\n",
    "        ])\n",
    "    \n",
    "    video_to_labeled_pos_intervals = {\n",
    "        v: ranges_to_intrvllist(v, labels)\n",
    "        for v, labels in ground_truth.positive.items()\n",
    "        if v in video_ids\n",
    "    }\n",
    "    video_to_labeled_neg_intervals = {\n",
    "        v: ranges_to_intrvllist(v, labels)\n",
    "        for v, labels in ground_truth.negative.items()\n",
    "        if v in video_ids\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    result = intrvllists_to_result(\n",
    "        video_to_anchor_intervals, color='green',\n",
    "        video_order=video_order)\n",
    "    add_intrvllists_to_result(result, video_to_context_intervals, color='orange')\n",
    "    add_intrvllists_to_result(result, video_to_topic_intervals, color='purple')\n",
    "    add_intrvllists_to_result(result, video_to_commerical_intervals, color='black')\n",
    "    add_intrvllists_to_result(result, video_to_labeled_pos_intervals, color='blue')\n",
    "    add_intrvllists_to_result(result, video_to_labeled_neg_intervals, color='red')\n",
    "    \n",
    "    video_widget = esper_widget(result, jupyter_keybindings=True,\n",
    "                                timeline_annotation_keys={';': 4, '\\'': 5},\n",
    "                                results_per_page=results_per_page)\n",
    "    update_button = widgets.Button(\n",
    "        description='Update ground truth',\n",
    "        disabled=False,\n",
    "        button_style='warning'\n",
    "    )\n",
    "    def on_update(b):\n",
    "        selected_idxs = set(video_widget.selected)\n",
    "        ignored_idxs = set(video_widget.ignored)\n",
    "        n_pos_segs = 0\n",
    "        n_neg_segs = 0\n",
    "        \n",
    "        def segment_is_ok(seg):\n",
    "            return 'min_frame' in seg and 'max_frame' in seg\n",
    "        \n",
    "        for i, video_id in enumerate(video_order):\n",
    "            video_fps = video_to_fps[video_id]\n",
    "            \n",
    "            pos_segments = []\n",
    "            neg_segments = []\n",
    "            if len(video_widget.groups) > 0:\n",
    "                pos_segments.extend([\n",
    "                    (\n",
    "                        int(seg['min_frame']) / video_fps, \n",
    "                        int(seg['max_frame']) / video_fps\n",
    "                    )\n",
    "                    for seg in video_widget.groups[i]['elements'][4]['segments'] \n",
    "                    if segment_is_ok(seg)\n",
    "                ])\n",
    "                neg_segments.extend([\n",
    "                    (\n",
    "                        int(seg['min_frame']) / video_fps, \n",
    "                        int(seg['max_frame']) / video_fps\n",
    "                    )\n",
    "                    for seg in video_widget.groups[i]['elements'][5]['segments'] \n",
    "                    if segment_is_ok(seg)\n",
    "                ])\n",
    "                \n",
    "            if i in selected_idxs:\n",
    "                pos_segments.extend([\n",
    "                    (p.start, p.end)\n",
    "                    for p in topic_results.video_to_segments[video_id]\n",
    "                ])\n",
    "            if i in ignored_idxs:\n",
    "                neg_segments.extend([\n",
    "                    (p.start, p.end)\n",
    "                    for p in topic_results.video_to_segments[video_id]\n",
    "                ])\n",
    "            \n",
    "            n_pos_segs += len(pos_segments)\n",
    "            if len(pos_segments) > 0:\n",
    "                if video_id not in ground_truth.positive:\n",
    "                    ground_truth.positive[video_id] = set()\n",
    "                ground_truth.positive[video_id].update(pos_segments)\n",
    "\n",
    "            n_neg_segs += len(neg_segments)\n",
    "            if len(neg_segments) > 0:\n",
    "                if video_id not in ground_truth.negative:\n",
    "                    ground_truth.negative[video_id] = set()\n",
    "                ground_truth.negative[video_id].update(neg_segments)\n",
    "\n",
    "        clear_output()\n",
    "        print('Added {} positive segments and {} negative segments.'.format(\n",
    "            n_pos_segs, n_neg_segs))\n",
    "                \n",
    "    update_button.on_click(on_update)\n",
    "    display(update_button)\n",
    "    display(video_widget)\n",
    "\n",
    "def show_lexicon_widgets():\n",
    "    status_output = widgets.Output()\n",
    "    anchor_widget = widgets.Textarea(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value='',\n",
    "        layout=widgets.Layout(width='100%'),\n",
    "        placeholder='Phrases (one per line)',\n",
    "        description='Anchor phrases:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global sync_anchor_widget\n",
    "    def sync_anchor_widget():\n",
    "        anchor_widget.value = '\\n'.join(sorted(ANCHOR_WORDS))\n",
    "        computed_height = 20 * (len(ANCHOR_WORDS) + 2)\n",
    "        anchor_widget.layout = widgets.Layout(\n",
    "            width='100%', \n",
    "            height='{}px'.format(computed_height)\n",
    "        )\n",
    "    def on_anchor_changed(b):\n",
    "        with status_output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                global ANCHOR_WORDS\n",
    "                ANCHOR_WORDS = {\n",
    "                    t.strip() for t in anchor_widget.value.split('\\n')\n",
    "                    if len(t.strip()) > 0\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    anchor_widget.observe(on_anchor_changed, names='value')\n",
    "\n",
    "    context_widget = widgets.Textarea(\n",
    "        value='',\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        layout=widgets.Layout(width='100%'),\n",
    "        placeholder='Phrases (one per line)',\n",
    "        description='Context phrases:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global sync_context_widget\n",
    "    def sync_context_widget():\n",
    "        context_widget.value = '\\n'.join(sorted(CONTEXT_WORDS))\n",
    "        max_height = 250\n",
    "        computed_height = 20 * (len(CONTEXT_WORDS) + 2)\n",
    "        context_widget.layout = widgets.Layout(\n",
    "            width='100%', \n",
    "            height='{}px'.format(min(max_height, computed_height))\n",
    "        )\n",
    "    def on_context_changed(b):\n",
    "        with status_output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                global CONTEXT_WORDS\n",
    "                CONTEXT_WORDS = {\n",
    "                    t.strip() for t in context_widget.value.split('\\n') \n",
    "                    if len(t.strip()) > 0\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    context_widget.observe(on_context_changed, names='value')\n",
    "\n",
    "    sort_button = widgets.Button(\n",
    "        description='Sort phrases',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_sort(b):\n",
    "        sync_anchor_widget()\n",
    "        sync_context_widget()\n",
    "    sort_button.on_click(on_sort)\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='Search for segments',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    global on_search\n",
    "    def on_search(b):\n",
    "        with status_output:\n",
    "            clear_output()\n",
    "            global TOPIC_SEGMENTS\n",
    "            TOPIC_SEGMENTS = find_segments(ANCHOR_WORDS, CONTEXT_WORDS)\n",
    "    search_button.on_click(on_search)\n",
    "\n",
    "    display(anchor_widget)\n",
    "    display(context_widget)\n",
    "    display(widgets.HBox([sort_button, search_button]))\n",
    "    display(status_output)\n",
    "    sync_anchor_widget()\n",
    "    sync_context_widget()\n",
    "\n",
    "def show_filter_widgets():\n",
    "    channel_filter_button = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'CNN', 'FOXNEWS', 'MSNBC'],\n",
    "        value='All',\n",
    "        description='Channel:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    canonical_show_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All'] + list(sorted(MAJOR_CANONICAL_SHOWS)),\n",
    "        value='All',\n",
    "        description='Show:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    start_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Start date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    end_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='End date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global FILTER_WIDGETS\n",
    "    FILTER_WIDGETS = {\n",
    "        'show': canonical_show_dropdown,\n",
    "        'channel': channel_filter_button,\n",
    "        'start_date': start_date_picker,\n",
    "        'end_date': end_date_picker\n",
    "    }\n",
    "    display(widgets.HBox([\n",
    "        channel_filter_button, canonical_show_dropdown, \n",
    "        start_date_picker, end_date_picker]))\n",
    "    \n",
    "def get_filters():\n",
    "    filters = {}\n",
    "    show = FILTER_WIDGETS['show'].value\n",
    "    if show != 'All':\n",
    "        filters['show'] = show\n",
    "    channel = FILTER_WIDGETS['channel'].value\n",
    "    if channel != 'All':\n",
    "        filters['channel'] = channel\n",
    "    if FILTER_WIDGETS['start_date'].value:\n",
    "        filters['start'] = FILTER_WIDGETS['start_date'].value\n",
    "    if FILTER_WIDGETS['end_date'].value:\n",
    "        filters['end'] = FILTER_WIDGETS['end_date'].value \n",
    "    return filters\n",
    "\n",
    "def show_video_controls():\n",
    "    show_videos_output = widgets.Output()\n",
    "    limit_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=1000,\n",
    "        min=1,\n",
    "        max=10000,\n",
    "        description='Video limit:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    results_per_page_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        description='Results per page:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    show_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Show videos',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    filter_videos_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'Unlabeled', 'Labeled Positive', 'Labeled Negative'],\n",
    "        value='All',\n",
    "        description='Videos:',\n",
    "        disabled=False\n",
    "    )\n",
    "    def on_show_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "            if filter_videos_dropdown.value == 'All':\n",
    "                exclude = set()\n",
    "            elif filter_videos_dropdown.value == 'Unlabeled':\n",
    "                exclude = set(GROUND_TRUTH.positive.keys()) | set(GROUND_TRUTH.negative.keys())\n",
    "            elif filter_videos_dropdown.value == 'Labeled Positive':\n",
    "                exclude = {\n",
    "                    x for x in TOPIC_SEGMENTS.video_to_segments\n",
    "                    if x not in GROUND_TRUTH.positive\n",
    "                }\n",
    "            elif filter_videos_dropdown.value == 'Labeled Negative':\n",
    "                exclude = {\n",
    "                    x for x in TOPIC_SEGMENTS.video_to_segments \n",
    "                    if x not in GROUND_TRUTH.negative\n",
    "                }\n",
    "            else:\n",
    "                raise Exception('Unknown option...')\n",
    "            display_segments(\n",
    "                TOPIC_SEGMENTS, GROUND_TRUTH, get_filters(),\n",
    "                limit=limit_slider.value,\n",
    "                results_per_page=results_per_page_slider.value,\n",
    "                exclude=exclude\n",
    "            )\n",
    "    show_videos_button.on_click(on_show_videos)\n",
    "    clear_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Dismiss videos',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_clear_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "    clear_videos_button.on_click(on_clear_videos)\n",
    "    display(widgets.HBox([\n",
    "        limit_slider, results_per_page_slider, filter_videos_dropdown]))\n",
    "    display(widgets.HBox([show_videos_button, clear_videos_button]))\n",
    "    display(show_videos_output)\n",
    "    \n",
    "STORY_DIRECTORY = '/app/data/stories/'\n",
    "if not os.path.isdir(STORY_DIRECTORY):\n",
    "    os.makedirs(STORY_DIRECTORY)\n",
    "\n",
    "def save_notebook_state():\n",
    "    name = input('Enter a story name: ').strip().replace(' ', '_')\n",
    "    assert name != '', 'Name cannot be empty'\n",
    "    out_path = os.path.join(STORY_DIRECTORY, '{}.json'.format(name))\n",
    "    if os.path.exists(out_path):\n",
    "        if input(\n",
    "            'File: {} already exists. Overwrite (y/N)? '.format(out_path)\n",
    "        ).strip().lower() != 'y':\n",
    "            print('Canceled by user.')\n",
    "            return\n",
    "\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'anchor_words': list(ANCHOR_WORDS),\n",
    "            'context_words': list(CONTEXT_WORDS),\n",
    "            'ground_truth': {\n",
    "                'positive_labels': {\n",
    "                    k: list(v) for k, v in GROUND_TRUTH.positive.items()\n",
    "                },\n",
    "                'negative_labels': {\n",
    "                    k: list(v) for k, v in GROUND_TRUTH.negative.items()\n",
    "                },\n",
    "            }\n",
    "        }, f)\n",
    "    print('Saved:', out_path)\n",
    "    \n",
    "def load_notebook_state():\n",
    "    print('The following stories are saved:')\n",
    "    for fname in sorted(os.listdir(STORY_DIRECTORY)):\n",
    "        print('', fname.split('.')[0].replace('_', ' '))\n",
    "    \n",
    "    name = input('Enter a story to load: ').strip().replace(' ', '_')\n",
    "    in_path = os.path.join(STORY_DIRECTORY, '{}.json'.format(name))\n",
    "    with open(in_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    global ANCHOR_WORDS, CONTEXT_WORDS, GROUND_TRUTH\n",
    "    ANCHOR_WORDS = set(data['anchor_words'])\n",
    "    CONTEXT_WORDS = set(data['context_words'])\n",
    "    GROUND_TRUTH = GroundTruth(\n",
    "        {int(k): set(tuple(y) for y in v) \n",
    "         for k, v in data['ground_truth']['positive_labels'].items()},\n",
    "        {int(k): set(tuple(y) for y in v) \n",
    "         for k, v in data['ground_truth']['negative_labels'].items()}\n",
    "    )\n",
    "    print('Loaded:', in_path)\n",
    "    sync_context_widget()\n",
    "    sync_anchor_widget()\n",
    "    on_search(None)\n",
    "\n",
    "try:\n",
    "    _FACE_IDENTS\n",
    "except NameError:\n",
    "    _FACE_IDENTS = None\n",
    "def get_face_idents():\n",
    "    global _FACE_IDENTS\n",
    "    if _FACE_IDENTS is None:\n",
    "        print('Loading face identities...', file=sys.stderr)\n",
    "        with open('/app/data/identities_by_video.pkl', 'rb') as f:\n",
    "            _FACE_IDENTS = pickle.load(f)\n",
    "    else:\n",
    "        pass\n",
    "    return _FACE_IDENTS\n",
    "\n",
    "try:\n",
    "    _FACE_GENDERS\n",
    "except NameError:\n",
    "    _FACE_GENDERS = None\n",
    "def get_face_genders():\n",
    "    global _FACE_GENDERS\n",
    "    if _FACE_GENDERS is None:\n",
    "        print('Loading face genders...', file=sys.stderr)\n",
    "        with open('/app/data/face_genders_by_video.pkl', 'rb') as f:\n",
    "            _FACE_GENDERS = pickle.load(f)\n",
    "    else:\n",
    "        pass\n",
    "    return _FACE_GENDERS\n",
    "    \n",
    "def init_global_variables():\n",
    "    global ANCHOR_WORDS, CONTEXT_WORDS\n",
    "    try:\n",
    "        ANCHOR_WORDS, CONTEXT_WORDS\n",
    "    except NameError:\n",
    "        ANCHOR_WORDS = set()\n",
    "        CONTEXT_WORDS = set()\n",
    "\n",
    "    global TOPIC_SEGMENTS, GROUND_TRUTH\n",
    "    try:\n",
    "        TOPIC_SEGMENTS, GROUND_TRUTH\n",
    "    except NameError:\n",
    "        TOPIC_SEGMENTS = None\n",
    "        GROUND_TRUTH = GroundTruth({}, {})\n",
    "    \n",
    "init_global_variables()\n",
    "\n",
    "print('Done initializing notebook.', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Some constants to help with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T22:05:53.325Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORD_WINDOW_SIZE = 5\n",
    "CONTEXT_WORD_EXTEND_THRESH = 120\n",
    "MIN_PROPOSED_SEGMENT_LEN = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Stories from a Lexicon\n",
    "\n",
    "Stories are retreived via lexicons of words. Story lexicons have two components, <b>anchor phrases</b> and <b>context phrases</b>.\n",
    "\n",
    "<b>Anchor phrases</b> are phrases that must appear for a segment to be considered a part of a story and should be unique to the story. For instance, a segment about 'Hurricane Irma' must mention either 'Irma' or 'Hurricane Irma'.\n",
    "\n",
    "<b>Context phrases</b> are phrases that relevant to the story, but not unique to it. For instance, words such as 'devastation' and 'storm' will be used in the context of 'Hurricane Irma' but also in context of other hurricanes and weather disasters. Presence of these words are used to prioritize the order in which results are presented.\n",
    "\n",
    "<b>Instructions:</b>\n",
    "- Enter anchor phrases to start (required; see caption-index query syntax)\n",
    "- Enter a few context phrases (optional)\n",
    "- Hit 'search for segments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T22:05:53.326Z"
    },
    "hide_input": false,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_lexicon_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Automatically propose context words</b>\n",
    "\n",
    "Once we have some segments corresponding to the lexicon, we can use NLP to propose new context words to improve story coverage. `propose_context_words()` will use statistics to suggest new lexicon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T04:15:57.033943Z",
     "start_time": "2019-02-22T04:15:55.148211Z"
    }
   },
   "outputs": [],
   "source": [
    "propose_context_words(TOPIC_SEGMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Filters\n",
    "You can apply the following filters to the retreived segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T22:05:53.328Z"
    },
    "hide_input": false,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_filter_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Videos\n",
    "\n",
    "Show videos and retreived topic segments with a timeline. You must have hit 'search for segments' prior to running this.\n",
    "\n",
    "Timeline colors:\n",
    "- Green = anchor phrases\n",
    "- Orange = context phrases\n",
    "- Purple = proposed story segment\n",
    "- Grey = commercial\n",
    "\n",
    "Timeline (Human Labeled) colors:\n",
    "- Blue = labeled positive segment\n",
    "- Red = labeled negative segment\n",
    "\n",
    "Videos will be ordered by descending amount of proposed time identified as the story.\n",
    "\n",
    "Select postive segments with <b>;</b> and negative segments with <b>'</b>. Use <b>[</b> and <b>]</b> to accept or reject all proposed story segments in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T22:05:53.330Z"
    },
    "hide_input": false,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_video_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Run `analysis()` to compute statistics over the story segments retreived. These graphs will respond to the filters earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T22:05:53.332Z"
    },
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def convert_ground_truth_to_topic_results(ground_truth):\n",
    "    return TopicSegments({\n",
    "        k: [\n",
    "          CaptionIndex.Posting(a, b, None, None) for a, b in v  \n",
    "        ] for k, v in ground_truth.positive.items()\n",
    "    }, None, None)\n",
    "\n",
    "def analysis(topic_results, filters={}):\n",
    "    video_qs = Video.objects.filter(\n",
    "        id__in=list(topic_results.video_to_segments.keys()), \n",
    "        duplicate=False)\n",
    "    video_qs = filter_video_qs(video_qs, filters)\n",
    "    video_to_meta = {\n",
    "        v['id']: {\n",
    "            'channel': v['channel__name'],\n",
    "            'show': v['show__canonical_show__name'],\n",
    "            'time': v['time'],\n",
    "            'fps': v['fps'],\n",
    "            'is_3y': v['threeyears_dataset'],\n",
    "            'path': v['path']\n",
    "        } for v in video_qs.values(\n",
    "            'id', 'channel__name', 'show__canonical_show__name', 'time', 'fps',\n",
    "            'threeyears_dataset', 'path'\n",
    "        )\n",
    "    }\n",
    "    if len(video_to_meta) == 0:\n",
    "        print('No videos to analyze. Please check the filters.',\n",
    "              file=sys.stderr)\n",
    "        return\n",
    "    \n",
    "    clear_button = widgets.Button(\n",
    "        description='Clear Analysis',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_clear(b):\n",
    "        clear_output()\n",
    "    clear_button.on_click(on_clear)\n",
    "    display(clear_button)\n",
    "    \n",
    "    channels = [c.name for c in Channel.objects.all()]\n",
    "    utc = timezone('UTC')\n",
    "    eastern = timezone('US/Eastern')\n",
    "    \n",
    "    channel_to_time = {c: 0. for c in channels}\n",
    "    channel_to_daypart_to_time = {c: np.zeros(24) for c in channels}\n",
    "    channel_to_weekday_to_time = {c: np.zeros(7) for c in channels}\n",
    "    channel_to_time_to_time = {c: defaultdict(float) for c in channels}\n",
    "    show_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "\n",
    "        video_topic_len = sum(p.end - p.start for p in postings)\n",
    "        channel = video_to_meta[video_id]['channel']\n",
    "        channel_to_time[channel] += video_topic_len\n",
    "        \n",
    "        video_dt = utc.localize(video_to_meta[video_id]['time']).astimezone(eastern)\n",
    "        for p in postings:\n",
    "            base_hour = video_dt.hour\n",
    "            posting_len = p.end - p.start\n",
    "            channel_to_daypart_to_time[channel][\n",
    "                (base_hour + int(p.start / 3600)) % 24\n",
    "            ] += posting_len\n",
    "            \n",
    "        channel_to_weekday_to_time[channel][video_dt.weekday()] += video_topic_len\n",
    "        channel_to_time_to_time[channel][video_dt.date()] += video_topic_len\n",
    "        \n",
    "        show = video_to_meta[video_id]['show']\n",
    "        show_to_time[(channel, show)] += video_topic_len\n",
    "        \n",
    "    print('Topic time by channel:')\n",
    "    for c in channel_to_time:\n",
    "        print('  {}: {:0.3f} hours'.format(c, channel_to_time[c] / 3600))\n",
    "        \n",
    "    print('\\nTopic time by day:')\n",
    "    def plot_timeline():\n",
    "        plt.figure(figsize=(11, 3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for c in channels:\n",
    "            data = [x for x in sorted(channel_to_time_to_time[c].items())]\n",
    "            plt.scatter(\n",
    "                [x for x, _ in data], [y / 60 for _, y in data],\n",
    "                alpha=0.5, s=2, label=c)\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Day')\n",
    "        plt.show()\n",
    "    plot_timeline()\n",
    "        \n",
    "    print('\\nTopic time by daypart:')\n",
    "    def plot_daypart():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(24) + (i - 1) * bar_width,\n",
    "                    channel_to_daypart_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(24))\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.show()\n",
    "    plot_daypart()\n",
    "    \n",
    "    print('\\nTopic time by weekday:')\n",
    "    def plot_weekday():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(7) + (i - 1) * bar_width, \n",
    "                    channel_to_weekday_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.show()\n",
    "    plot_weekday()\n",
    "    \n",
    "    top_n = 10\n",
    "    print('\\nShows with most coverage (top-{}):'.format(top_n))\n",
    "    for (channel, show), seconds in show_to_time.most_common(top_n):\n",
    "        print('  {} ({}): {:0.1f} minutes'.format(show, channel, seconds / 60))\n",
    "\n",
    "    def join_face_labels_and_postings(labels, postings, label_len=3):\n",
    "        result = Counter()\n",
    "        try:\n",
    "            label_head = next(labels)\n",
    "            postings_head = next(postings)\n",
    "            while True:\n",
    "                if label_head[1] > postings_head.end:\n",
    "                    postings_head = next(postings)\n",
    "                elif label_head[1] + label_len < postings_head.start:\n",
    "                    label_head = next(labels)\n",
    "                else:\n",
    "                    result[label_head[0]] += label_len\n",
    "                    label_head = next(labels)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        return result\n",
    "    \n",
    "    # Time by Gender\n",
    "    face_genders = get_face_genders()\n",
    "    gender_to_time = Counter()\n",
    "    gender_to_time_host = Counter()\n",
    "    channel_to_gender_to_time = defaultdict(lambda: Counter())\n",
    "    channel_to_gender_to_time_host = defaultdict(lambda: Counter())\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "        video_genders = face_genders.get(video_id, [])\n",
    "        \n",
    "        # Compute for all faces\n",
    "        video_story_genders = join_face_labels_and_postings(\n",
    "            iter(video_genders), iter(postings))\n",
    "        gender_to_time.update(video_story_genders)\n",
    "        channel_to_gender_to_time[\n",
    "            video_to_meta[video_id]['channel']\n",
    "        ].update(video_story_genders)\n",
    "        \n",
    "        # Compute for hosts only\n",
    "        video_story_genders_host = join_face_labels_and_postings(\n",
    "            filter(lambda x: x[-1], video_genders), iter(postings))\n",
    "        gender_to_time_host.update(video_story_genders_host)\n",
    "        channel_to_gender_to_time_host[\n",
    "            video_to_meta[video_id]['channel']\n",
    "        ].update(video_story_genders_host)\n",
    "    \n",
    "    def plot_gender_screen_time(data):\n",
    "        male_props = []\n",
    "        totals = []\n",
    "        for name, gender_to_time in data:\n",
    "            total = sum(gender_to_time[k] for k in gender_to_time)\n",
    "            male_prop = gender_to_time[1] / total\n",
    "            male_props.append(male_prop)\n",
    "            totals.append(total)\n",
    "        \n",
    "        x = np.arange(len(data))\n",
    "        names = [x[0] for x in data]\n",
    "        male_props = np.array(male_props)\n",
    "        totals = np.array(totals)\n",
    "        width = 0.5\n",
    "        \n",
    "        plt.figure(figsize=(11,3))\n",
    "        p1 = plt.bar(x, male_props, width, color='lightblue',\n",
    "                     label='Men')\n",
    "        p2 = plt.bar(x, (male_props - 1.), width,\n",
    "                     color='salmon', label='Women')\n",
    "        plt.axhline(color='black')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.xticks(x, names, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    print('\\nFace screen time by gender:')\n",
    "    gender_screen_time_data = []\n",
    "    gender_screen_time_data.append(('All channels', gender_to_time))\n",
    "    for channel in channel_to_gender_to_time:\n",
    "        gender_screen_time_data.append((channel, channel_to_gender_to_time[channel]))\n",
    "    gender_screen_time_data.append(('All channels (hosts)', gender_to_time_host))\n",
    "    for channel in channel_to_gender_to_time:\n",
    "        gender_screen_time_data.append(('{} (hosts)'.format(channel), \n",
    "                                        channel_to_gender_to_time_host[channel]))\n",
    "    gender_screen_time_data.append(('All channels (non-hosts)', \n",
    "                                    gender_to_time - gender_to_time_host))\n",
    "    for channel in channel_to_gender_to_time:\n",
    "        gender_screen_time_data.append(('{} (non-hosts)'.format(channel), \n",
    "                                        channel_to_gender_to_time[channel] \n",
    "                                        - channel_to_gender_to_time_host[channel]))\n",
    "        \n",
    "    plot_gender_screen_time(gender_screen_time_data)\n",
    "        \n",
    "    # Time by Identity\n",
    "    face_idents = get_face_idents()\n",
    "    ident_id_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "        video_idents = face_idents.get(video_id, [])\n",
    "        ident_id_to_time.update(join_face_labels_and_postings(iter(video_idents), iter(postings)))\n",
    "    top_n = 10\n",
    "    print('\\nPeople with most screen time (top-{}):'.format(top_n))\n",
    "    for ident_id, seconds in ident_id_to_time.most_common(top_n):\n",
    "        print('  {}: {:0.1f} minutes'.format(\n",
    "            Identity.objects.get(id=ident_id).name, \n",
    "            seconds / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T04:36:20.657327Z",
     "start_time": "2019-02-22T04:33:59.345691Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis(TOPIC_SEGMENTS, get_filters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T05:15:01.749959Z",
     "start_time": "2019-02-22T05:14:39.281317Z"
    }
   },
   "outputs": [],
   "source": [
    "analysis(convert_ground_truth_to_topic_results(GROUND_TRUTH), get_filters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Loading Progress\n",
    "\n",
    "Save your progress. Locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T05:48:52.488060Z",
     "start_time": "2019-02-22T05:48:40.715383Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "save_notebook_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T06:47:31.405230Z",
     "start_time": "2019-02-22T06:47:26.526962Z"
    }
   },
   "outputs": [],
   "source": [
    "load_notebook_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Demo Code\n",
    "Load a debugging lexicon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T18:33:11.151027Z",
     "start_time": "2019-02-21T18:33:06.025680Z"
    }
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORDS = {\n",
    "    'HURRICANE & IRMA :: 30'\n",
    "}\n",
    "CONTEXT_WORDS = { \n",
    "    'ADVISORY', 'ATLANTIC', 'BANDS', 'BEACH', 'BOATS', 'BRACING', 'BRIDGES',\n",
    "    'CARIBBEAN', 'CATASTROPHIC', 'CATEGORY', 'CLEANUP', 'COAST', 'COASTAL',\n",
    "    'CUBA', 'DAMAGE', 'DEBRIS', 'DESTRUCTION', 'DESTRUCTIVE', 'DEVASTATED',\n",
    "    'DEVASTATING', 'DEVASTATION', 'DISASTERS', 'DOWNTOWN', 'ELECTRICITY',\n",
    "    'EVACUATE', 'EVACUATED', 'EVACUATION', 'EVACUATIONS', 'FEMA', 'FLOOD',\n",
    "    'FLOODED', 'FLOODING', 'FLORIDA', 'FORECAST', 'GUSTS', 'HARVEY', 'HURRICANE',\n",
    "    'HURRICANES', 'IMPACTED', 'IMPACTS', 'INTENSITY', 'IRMA', 'ISLAND', 'ISLANDS',\n",
    "    'JOSE', 'KEYS', 'LANDFALL', 'MANDATORY', 'METEOROLOGIST', 'MIAMI', 'MONSTER',\n",
    "    'MYERS', 'NURSING', 'ORLANDO', 'OUTAGES', 'OUTER', 'PALM', 'POWER',\n",
    "    'PREPARATION', 'PUERTO', 'RAIN', 'RAINFALL', 'RAINS', 'REBUILD',\n",
    "    'RESPONDERS', 'RESTORED', 'RICO', 'SHELTER', 'SHELTERS', 'STORM',\n",
    "    'STORMS', 'STRONGEST', 'SUPPLIES', 'SURGE', 'SUSTAINED', 'TAMPA',\n",
    "    'TIDE', 'TREES', 'TROPICAL', 'WARNINGS', 'WATER', 'WAVES', 'WIND', 'WINDS'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_anchor_widget()\n",
    "on_search(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T21:43:36.735450Z",
     "start_time": "2019-02-21T21:43:35.362664Z"
    }
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORDS = {\n",
    "    'MOSUL & (BATTLE | SIEGE) :: 60'\n",
    "}\n",
    "CONTEXT_WORDS = {\n",
    "    'MOSUL', 'COMMANDERS', 'KURDISH', 'STRATEGIC', 'EXPLOSIONS', 'ENEMY',\n",
    "    'BOMBERS', 'GUNFIRE', 'CIVILIANS', 'OFFENSIVE', 'OPERATION', 'KURDS',\n",
    "    'DEFEAT', 'FIERCE', 'IRAQIS', 'PROVINCE', 'EXPLOSIVES', 'BAGHDAD',\n",
    "    'URBAN', 'BATTLES', 'DAM', 'ISIL', 'RETREAT', 'ISIS', 'COMBAT',\n",
    "    'SURROUNDED', 'TERRITORY', 'DECISIVE', 'STRIKES', 'CIVILIAN', 'OPERATIONS',\n",
    "    'BOMBINGS', 'FLEEING', 'SUNNI', 'BATTLE', 'FLEE', 'ARMY', 'COALITION',\n",
    "    'FIGHTING', 'BATTLEFIELD', 'FLED', 'CASUALTIES', 'FIGHTERS', 'IRAQI',\n",
    "    'DEFEATED', 'BOMBS', 'FORCES', 'TROOPS', 'TUNNELS', 'SIEGE', 'MILITIA',\n",
    "    'MILITANTS', 'TACTICAL', 'ARTILLERY', 'IRAQ', 'ISLAMIC', 'RESISTANCE',\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_anchor_widget()\n",
    "on_search(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T18:33:06.018896Z",
     "start_time": "2019-02-21T18:32:45.753362Z"
    }
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORDS = {\n",
    "    '(PARKLAND | STONEMAN DOUGLAS | FLORIDA) & SHOOTING :: 60'\n",
    "}\n",
    "CONTEXT_WORDS = {\n",
    "    'DEPUTIES', 'DEADLY', 'PARKLAND', 'HORRIFIC', 'FIREARMS', 'SHERIFF',\n",
    "    'DOUGLAS', 'GUN', 'STONEMAN', 'SURVIVOR', 'MASSACRE', 'SHOOTER',\n",
    "    'SHOOTINGS', 'FRESHMAN', 'RIFLES', 'MASS', 'CLASSES', 'SCHOOL',\n",
    "    'SHOT', 'SURVIVORS', 'SHOOTING', 'HIGH', 'FLORIDA', 'STUDENTS',\n",
    "    'TEACHERS', 'VICTIMS', 'SURVIVED', 'ORGANIZERS'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_anchor_widget()\n",
    "on_search(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T04:47:14.900173Z",
     "start_time": "2019-02-22T04:47:14.564946Z"
    }
   },
   "outputs": [],
   "source": [
    "ANCHOR_WORDS = {\n",
    "    'FIFA'\n",
    "}\n",
    "CONTEXT_WORDS = {\n",
    "    'ETHICS', 'INVESTIGATING', 'INDICTMENT', 'RESIGN', 'CORRUPTION',\n",
    "    'ARRESTS', 'PLEADED', 'ACCUSATION', 'SUSPENDED', 'RESIGNATION',\n",
    "    'INDICTED', 'ALLEGATION', 'SCANDAL', 'ALLEGATIONS', 'ARRESTED',\n",
    "    'SCANDALS', 'BRIBERY', 'RESIGNED', 'ABUSED', 'ACCUSATIONS', \n",
    "    'CHARGES', 'CORRUPT'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_anchor_widget()\n",
    "on_search(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T21:56:47.359025Z",
     "start_time": "2019-02-22T21:56:47.327449Z"
    }
   },
   "outputs": [],
   "source": [
    "GROUND_TRUTH = GroundTruth({}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
