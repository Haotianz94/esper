{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Install-gentle\" data-toc-modified-id=\"Install-gentle-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Install gentle</a></span></li><li><span><a href=\"#Compute-audio_duration,-bad_transcript,-missing_transcript\" data-toc-modified-id=\"Compute-audio_duration,-bad_transcript,-missing_transcript-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Compute audio_duration, bad_transcript, missing_transcript</a></span></li><li><span><a href=\"#Test-aligning-transcript-with-local-video\" data-toc-modified-id=\"Test-aligning-transcript-with-local-video-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Test aligning transcript with local video</a></span></li><li><span><a href=\"#Test-aligning-transcript-using-scanner-pipeline\" data-toc-modified-id=\"Test-aligning-transcript-using-scanner-pipeline-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Test aligning transcript using scanner pipeline</a></span></li><li><span><a href=\"#Complete-transcript-loading\" data-toc-modified-id=\"Complete-transcript-loading-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Complete transcript loading</a></span></li><li><span><a href=\"#Analyze-result\" data-toc-modified-id=\"Analyze-result-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analyze result</a></span></li><li><span><a href=\"#Check-audio-time->-frame-time\" data-toc-modified-id=\"Check-audio-time->-frame-time-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Check audio time &gt; frame time</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install gentle \n",
    "gentle is a 3rd party library for audio-transcript aligment, we need gentle installed in the docker image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "git clone https://github.com/scanner-research/gentle\n",
    "\n",
    "2.\n",
    "bash ./install.sh (takes ~30min)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute audio_duration, bad_transcript, missing_transcript\n",
    "We need to pre-compute audio duration instead of video duration(number of frames) which will be used in the AudioSource kernel; We also want to pre-filter raw transcripts to make sure they are complete (longer than certain amount of the whole video) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esper.util import *\n",
    "import pickle\n",
    "from query.models import Video\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "# videos = Video.objects.filter(path__contains='2018')\n",
    "\n",
    "video_ids = open('/app/data/video_list_2019.txt', 'r').read().split('\\n')\n",
    "videos = Video.objects.filter(id__in=video_ids)\n",
    "len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 80\n",
    "pool = multiprocessing.Pool(num_threads)\n",
    "num_video_t = len(videos) // num_threads\n",
    "video_list_t = []\n",
    "for i in range(num_threads):\n",
    "    if i != num_threads - 1:\n",
    "        video_list_t.append(videos[i*num_video_t: (i+1)*num_video_t])\n",
    "    else:\n",
    "        video_list_t.append(videos[i*num_video_t:])\n",
    "\n",
    "pkl_path_list = pool.map(get_video_field_t, video_list_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_path_list = open('/app/tmp/pkl_path_list.txt', 'r').read().split()\n",
    "\n",
    "# result_final = pickle.load(open('/app/data/addtional_field_all.pkl', 'rb'))\n",
    "result_final = {}\n",
    "for pkl_path in pkl_path_list:\n",
    "    result = pickle.load(open(pkl_path, 'rb'))\n",
    "    for key, val in result.items():\n",
    "        result_final[key] = val\n",
    "print(len(result_final))\n",
    "pickle.dump(result_final, open('/app/data/addtional_field_2019.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pickle.load(open('/app/data/addtional_field_all.pkl', 'rb'))\n",
    "cnt = 0\n",
    "for key, res in result_final.items():\n",
    "    if not res['valid_transcript']:\n",
    "        cnt += 1\n",
    "print(cnt, cnt / len(result_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test aligning transcript with local video\n",
    "Test the transcript aligment class locally without using scanner at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T00:14:24.872057Z",
     "start_time": "2019-10-08T00:14:24.600141Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from query.models import Video\n",
    "import scannerpy\n",
    "import os\n",
    "\n",
    "get_ipython().magic('reload_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T22:05:56.340660Z",
     "start_time": "2019-10-01T22:05:44.625590Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = scannerpy.Database().summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T02:36:34.192640Z",
     "start_time": "2019-09-24T02:36:34.160895Z"
    }
   },
   "outputs": [],
   "source": [
    "# set test video list\n",
    "video_list = ['FOXNEWSW_20160604_140000_Cost_of_Freedom_Dash_to_the_Clash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:00:36.549819Z",
     "start_time": "2018-12-20T07:00:36.500018Z"
    }
   },
   "outputs": [],
   "source": [
    "# set srt extension\n",
    "for video_name in video_list:\n",
    "    video = Video.objects.filter(path__contains=video_name)[0]\n",
    "    video.srt_extension = 'word'\n",
    "    video.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:04:33.877161Z",
     "start_time": "2018-12-19T10:04:24.672313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scannertools.transcript_alignment import TranscriptAligner\n",
    "\n",
    "res_stats = {}\n",
    "for video_name in video_list:\n",
    "    print(video_name)\n",
    "    # download video\n",
    "    video_path = os.path.join('../data/videos/', video_name+'.mp4')\n",
    "    if not os.path.exists(video_path):\n",
    "        gs_path = os.path.join('gs://esper/tvnews/videos/', video_name+'.mp4')\n",
    "        cmd = 'gsutil cp ' + gs_path + ' ' + '../data/videos/'\n",
    "        os.system(cmd)\n",
    "    print('Downloading video done')\n",
    "    \n",
    "    # run alignment\n",
    "    aligner = TranscriptAligner(win_size=300, seg_length=60, max_misalign=10, num_thread=64, estimate=True,\n",
    "                            media_path=video_path,\n",
    "                            transcript_path=os.path.join('/app/data/subs10/', video_name),\n",
    "                            align_dir='/app/data/subs/orig/')\n",
    "    res = aligner.run_all()\n",
    "#     res_stats[video_name] = res\n",
    "#     pickle.dump(res_stats, open('/app/result/test_align_100_hard.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test aligning transcript using scanner pipeline\n",
    "Test the transcript aligment using scanner pipeline. Please also refer to /app/esper/transcript_aligment.py for more instructions. We use that script to run large jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T17:52:37.351793Z",
     "start_time": "2019-09-24T17:52:37.291877Z"
    }
   },
   "outputs": [],
   "source": [
    "from scannertools import audio\n",
    "from scannertools.transcript_alignment import align_transcript_pipeline\n",
    "from tqdm import tqdm\n",
    "SEG_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T17:52:44.419141Z",
     "start_time": "2019-09-24T17:52:44.256363Z"
    }
   },
   "outputs": [],
   "source": [
    "# set test video list\n",
    "video_list = ['FOXNEWSW_20160604_140000_Cost_of_Freedom_Dash_to_the_Clash']\n",
    "videos = [Video.objects.filter(path__contains=video_name)[0] for video_name in video_list]\n",
    "\n",
    "# videos = Video.objects.filter(threeyears_dataset=True).all()\n",
    "addtional_field = pickle.load(open('/app/data/addtional_field_2019.pkl', 'rb'))\n",
    "# videos = [video for video in videos if addtional_field[video.id]['valid_transcript']]\n",
    "# videos = videos[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T17:53:02.327493Z",
     "start_time": "2019-09-24T17:52:50.252999Z"
    }
   },
   "outputs": [],
   "source": [
    "# check database\n",
    "db = scannerpy.Database()\n",
    "videos_committed = []\n",
    "for video in tqdm(videos):\n",
    "    table_name = '{}_align_transcript'.format(video.path)\n",
    "    table = db.table(table_name)\n",
    "    if not table.committed():\n",
    "        print(video.item_name())\n",
    "    else:\n",
    "        videos_committed.append(video)\n",
    "videos = videos_committed\n",
    "len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T17:55:09.744653Z",
     "start_time": "2019-09-24T17:55:09.675109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load audios from videos\n",
    "audios = [audio.AudioSource(video.for_scannertools(), \n",
    "                                frame_size=SEG_LENGTH, \n",
    "                                duration=addtional_field[video.id]['audio_duration']) \n",
    "              for video in videos]\n",
    "\n",
    "# set up transcripts \n",
    "captions = [audio.CaptionSource('tvnews/subs2019/'+video.item_name(), \n",
    "                                max_time=addtional_field[video.id]['audio_duration'] , \n",
    "                                window_size=SEG_LENGTH) \n",
    "            for video in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:15:40.097376Z",
     "start_time": "2019-09-24T18:15:40.031649Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up run opts\n",
    "run_opts = {'pipeline_instances_per_node': 96, 'checkpoint_frequency': 5}\n",
    "\n",
    "# set up align opts\n",
    "align_opts = {'win_size': 300,\n",
    "                  'seg_length' : 60,\n",
    "                  'max_misalign' : 10,\n",
    "                  'num_thread' : 1,\n",
    "                  'estimate' : False,\n",
    "                  'align_dir' : None,\n",
    "                  'res_path' : None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:22:59.815792Z",
     "start_time": "2019-09-24T18:15:42.034056Z"
    }
   },
   "outputs": [],
   "source": [
    "result = align_transcript_pipeline(db=db, audio=audios, captions=captions, cache=False, \n",
    "                                                   run_opts=run_opts, align_opts=align_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete transcript loading\n",
    "Before this step, run /app/esper/transcript_aligment.py with BATCH_LOAD to dump results from scanner database. Then use this part to collect staus of the aligment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:55:13.659681Z",
     "start_time": "2019-10-09T18:55:13.571083Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:55:16.310045Z",
     "start_time": "2019-10-09T18:55:16.277668Z"
    }
   },
   "outputs": [],
   "source": [
    "updates = open('/app/tmp/align_new_6.log', 'r').read().split('\\n')\n",
    "print(updates[:5])\n",
    "\n",
    "updates = [line.split(' ') for line in updates[5:-1]]\n",
    "len(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:55:20.858733Z",
     "start_time": "2019-10-09T18:55:20.812412Z"
    }
   },
   "outputs": [],
   "source": [
    "res_stats = pickle.load(open('/app/result/align_stats_final_2019.pkl', 'rb'))\n",
    "# res_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:55:27.035905Z",
     "start_time": "2019-10-09T18:55:25.133253Z"
    }
   },
   "outputs": [],
   "source": [
    "for path, r in updates:\n",
    "    video = Video.objects.filter(path=path)[0]\n",
    "    res_stats[video.id] = {'word_missing': float(r)}\n",
    "len(res_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:56:21.446835Z",
     "start_time": "2019-10-09T18:56:21.404214Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(res_stats, open('/app/result/align_stats_final_2019.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:57:36.752965Z",
     "start_time": "2019-10-09T18:57:36.629870Z"
    }
   },
   "outputs": [],
   "source": [
    "video_ids = open('/app/data/video_list_2019.txt', 'r').read().split('\\n')\n",
    "videos = Video.objects.filter(id__in=video_ids)\n",
    "addtional_field = pickle.load(open('/app/data/addtional_field_2019.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:57:52.059021Z",
     "start_time": "2019-10-09T18:57:50.741125Z"
    }
   },
   "outputs": [],
   "source": [
    "for video in videos:\n",
    "    if video.id in res_stats and res_stats[video.id]['word_missing'] <= 0.2:\n",
    "        addtional_field[video.id]['aligned_transcript'] = True\n",
    "    else:\n",
    "        addtional_field[video.id]['aligned_transcript'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:57:55.182288Z",
     "start_time": "2019-10-09T18:57:55.144478Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(addtional_field, open('/app/data/addtional_field_2019.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T18:59:58.915384Z",
     "start_time": "2019-10-09T18:59:58.890749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for id in addtional_field:\n",
    "    if addtional_field[id]['aligned_transcript']:\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T20:30:27.400622Z",
     "start_time": "2019-10-09T20:29:34.743517Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/app/result/aligned2019.txt', 'w') as f:\n",
    "    for video_id in sorted(addtional_field):\n",
    "        video = Video.objects.filter(id=video_id)[0]\n",
    "        if addtional_field[video_id]['aligned_transcript']:\n",
    "            f.write(video.item_name() + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T00:19:27.485113Z",
     "start_time": "2018-12-23T00:19:27.281464Z"
    }
   },
   "outputs": [],
   "source": [
    "align_stats_first = pickle.load(open('/app/result/align_stats_first.pkl', 'rb'))\n",
    "align_stats_second = pickle.load(open('/app/result/align_stats_second.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T00:25:46.541073Z",
     "start_time": "2018-12-23T00:25:42.452434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videos = Video.objects.all()\n",
    "cnt = 0\n",
    "video_list = []\n",
    "clean_file = open('/app/result/clean_subs.txt', 'w')\n",
    "for video in videos:\n",
    "    if video.id in align_stats_second and align_stats_second[video.id]['word_missing'] <= 0.2:\n",
    "#         if cnt < 100:\n",
    "#             print(video.id, res_stats[video.id]['word_missing'])\n",
    "#             video_list.append(video.id)\n",
    "        cnt += 1\n",
    "        clean_file.write(video.item_name() + '\\n')\n",
    "print(len(videos), cnt)\n",
    "# print(video_list)\n",
    "clean_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T00:20:36.399543Z",
     "start_time": "2018-12-23T00:20:36.049859Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge second into first\n",
    "align_stats_final = {id: res for id, res in align_stats_first.items() if res['word_missing'] < 0.2}\n",
    "for id, res in align_stats_second.items():\n",
    "    align_stats_final[id] = res\n",
    "res_list_first = [res['word_missing'] if res['word_missing'] > 0 else 0 for id, res in align_stats_first.items() ]\n",
    "res_list_final = [res['word_missing'] if res['word_missing'] > 0 else 0 for id, res in align_stats_final.items() if res['word_missing'] < 1]\n",
    "res_list_first.sort()\n",
    "res_list_final.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T00:20:38.266908Z",
     "start_time": "2018-12-23T00:20:37.904730Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(res_list_first)\n",
    "plt.plot(res_list_final)\n",
    "plt.xlabel('num of videos')\n",
    "plt.ylabel('mis-aligned ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check audio time > frame time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T19:15:13.281141Z",
     "start_time": "2018-12-19T19:15:09.251332Z"
    }
   },
   "outputs": [],
   "source": [
    "# set test video list\n",
    "videos = Video.objects.all()\n",
    "addtional_field = pickle.load(open('/app/data/addtional_field_all.pkl', 'rb'))\n",
    "videos = [video for video in videos if addtional_field[video.id]['valid_transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T19:48:16.784571Z",
     "start_time": "2018-12-19T19:48:16.471354Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for video in videos:\n",
    "    audio_time = addtional_field[video.id]['audio_duration']\n",
    "    frame_time = video.num_frames / video.fps\n",
    "    if audio_time / frame_time > 1.1 or audio_time / frame_time < 0.9:\n",
    "        cnt += 1\n",
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
