{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Shows\" data-toc-modified-id=\"Shows-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Shows</a></span><ul class=\"toc-item\"><li><span><a href=\"#Show-Info\" data-toc-modified-id=\"Show-Info-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Show Info</a></span></li><li><span><a href=\"#Gender-By-Show\" data-toc-modified-id=\"Gender-By-Show-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Gender By Show</a></span></li><li><span><a href=\"#Identity-by-Show\" data-toc-modified-id=\"Identity-by-Show-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Identity by Show</a></span></li></ul></li><li><span><a href=\"#Topics\" data-toc-modified-id=\"Topics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Topics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-Lexicons\" data-toc-modified-id=\"Topic-Lexicons-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Topic Lexicons</a></span></li><li><span><a href=\"#Gender-By-Topic\" data-toc-modified-id=\"Gender-By-Topic-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Gender By Topic</a></span></li><li><span><a href=\"#Identity-by-Topic\" data-toc-modified-id=\"Identity-by-Topic-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Identity by Topic</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:16:11.670553Z",
     "start_time": "2018-10-07T22:16:07.109471Z"
    }
   },
   "outputs": [],
   "source": [
    "from esper.widget import *\n",
    "from esper.prelude import *\n",
    "from esper.spark_util import *\n",
    "from esper.major_canonical_shows import MAJOR_CANONICAL_SHOWS\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from datetime import timedelta, datetime\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:16:11.702133Z",
     "start_time": "2018-10-07T22:16:11.673807Z"
    }
   },
   "outputs": [],
   "source": [
    "OVERWRITE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:17:55.259682Z",
     "start_time": "2018-10-07T22:16:11.704565Z"
    }
   },
   "outputs": [],
   "source": [
    "face_genders = get_face_genders()\n",
    "face_genders = face_genders.where(face_genders.labeler_id != Labeler.objects.get(name='handlabeled-gender').id)\n",
    "face_genders = face_genders.withColumn('date', func.date_format('time', 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:19:02.279474Z",
     "start_time": "2018-10-07T22:17:55.261619Z"
    }
   },
   "outputs": [],
   "source": [
    "face_identities = get_face_identities()\n",
    "face_identities = face_identities.where(face_identities.labeler_id.isin(\n",
    "    [l.id for l in Labeler.objects.filter(name__contains='face-identity')]\n",
    "))\n",
    "face_identities = face_identities.withColumn('date', func.date_format('time', 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:19:02.329652Z",
     "start_time": "2018-10-07T22:19:02.282063Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_keys(d):\n",
    "    def tuple_to_str(t):\n",
    "        return ':'.join([str(x) for x in t])\n",
    "    return { tuple_to_str(k) : v for k, v in d.items() }\n",
    "\n",
    "def capitalize_name(s):\n",
    "    return ' '.join([x.capitalize() for x in s.split(' ')])\n",
    "\n",
    "identity_map = { x.id : capitalize_name(x.name) for x in Identity.objects.all() }\n",
    "\n",
    "canonical_show_map = {\n",
    "    c.id : c.name for c in CanonicalShow.objects.all() \n",
    "    if c.name in MAJOR_CANONICAL_SHOWS\n",
    "}\n",
    "\n",
    "channel_map = {\n",
    "    c.id : c.name for c in Channel.objects.all()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T03:19:13.507496Z",
     "start_time": "2018-10-08T03:19:13.386606Z"
    }
   },
   "outputs": [],
   "source": [
    "canonical_show_to_info = {\n",
    "    c : {\n",
    "        'channel': Video.objects.filter(show__canonical_show__name=c)[0].channel.name,\n",
    "        'aliases': [s.name for s in Show.objects.filter(canonical_show__name=c).order_by('name')] \n",
    "    }\n",
    "    for c in MAJOR_CANONICAL_SHOWS\n",
    "}\n",
    "\n",
    "SHOW_INFO_PATH = 'widget_data/show_info.json'\n",
    "if not OVERWRITE and os.path.exists(SHOW_INFO_PATH):\n",
    "    raise Exception('File exists!')\n",
    "    \n",
    "with open(SHOW_INFO_PATH, 'w') as f:\n",
    "    json.dump(canonical_show_to_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender By Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:22:14.789390Z",
     "start_time": "2018-10-07T22:19:02.331665Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_show_date(k):\n",
    "    cshow_id, channel_id, date = k\n",
    "    default_show = 'Other ({})'.format(channel_map[channel_id])\n",
    "    return (canonical_show_map.get(cshow_id, default_show), date)\n",
    "\n",
    "# Including host\n",
    "screen_time_male_by_show_date = {}\n",
    "for k, v in sum_over_column(\n",
    "    face_genders, 'duration', ['canonical_show_id', 'channel_id', 'date'],\n",
    "    probability_column='male_probability'\n",
    ").items():\n",
    "    screen_time_male_by_show_date[get_show_date(k)] = v\n",
    "\n",
    "screen_time_female_by_show_date = {}\n",
    "for k, v in sum_over_column(\n",
    "    face_genders, 'duration', ['canonical_show_id', 'channel_id', 'date'],\n",
    "    probability_column='female_probability'\n",
    ").items():\n",
    "    screen_time_female_by_show_date[get_show_date(k)] = v\n",
    "    \n",
    "# Exclude hosts\n",
    "face_genders_nh = face_genders.where(face_genders.host_probability < 0.5)\n",
    "\n",
    "screen_time_male_nh_by_show_date = {}\n",
    "for k, v in sum_over_column(\n",
    "    face_genders_nh, 'duration', ['canonical_show_id', 'channel_id', 'date'],\n",
    "    probability_column='male_probability'\n",
    ").items():\n",
    "    screen_time_male_nh_by_show_date[get_show_date(k)] = v\n",
    "\n",
    "screen_time_female_nh_by_show_date = {}\n",
    "for k, v in sum_over_column(\n",
    "    face_genders_nh, 'duration', ['canonical_show_id', 'channel_id', 'date'],\n",
    "    probability_column='female_probability'\n",
    ").items():\n",
    "    screen_time_female_nh_by_show_date[get_show_date(k)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:22:15.807545Z",
     "start_time": "2018-10-07T22:22:14.791218Z"
    }
   },
   "outputs": [],
   "source": [
    "GENDER_BY_SHOW_PATH = 'widget_data/gender_by_show.json'\n",
    "if not OVERWRITE and os.path.exists(GENDER_BY_SHOW_PATH):\n",
    "    raise Exception('File exists!')\n",
    "    \n",
    "with open(GENDER_BY_SHOW_PATH, 'w') as f:\n",
    "    json.dump({\n",
    "        'all': {\n",
    "            'male': json_keys(screen_time_male_by_show_date),\n",
    "            'female': json_keys(screen_time_female_by_show_date)\n",
    "        },\n",
    "        'nonhost': {\n",
    "            'male': json_keys(screen_time_male_nh_by_show_date),\n",
    "            'female': json_keys(screen_time_female_nh_by_show_date)\n",
    "        }\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity by Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:23:12.459020Z",
     "start_time": "2018-10-07T22:22:15.809388Z"
    }
   },
   "outputs": [],
   "source": [
    "screen_time_identity_by_show_date = defaultdict(dict)\n",
    "for k, v in sum_over_column(\n",
    "    face_identities, 'duration', ['identity_id', 'canonical_show_id', 'channel_id', 'date'],\n",
    "    probability_column='probability'\n",
    ").items():\n",
    "    if k[0] in identity_map:\n",
    "        screen_time_identity_by_show_date[identity_map[k[0]]][get_show_date(k[1:])] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:35:53.518697Z",
     "start_time": "2018-10-07T22:35:39.582028Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IDENTITY_BY_SHOW_PATH = 'widget_data/identity_by_show.json'\n",
    "if not OVERWRITE and os.path.exists(IDENTITY_BY_SHOW_PATH):\n",
    "    raise Exception('File exists!')\n",
    "    \n",
    "with open(IDENTITY_BY_SHOW_PATH, 'w') as f:\n",
    "    json.dump({\n",
    "        k : json_keys(v) \n",
    "        for k, v in screen_time_identity_by_show_date.items() \n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    'terrorism', 'isis', 'syria', 'refugee',\n",
    "    'collusion', 'russia',\n",
    "    'shooting', 'black lives', \n",
    "    'san bernardino', 'pulse nightclub', 'vegas shooter', 'charleston church',\n",
    "    'charlie hebdo', 'paris attacks',\n",
    "    'trayvon martin', 'walter scott',\n",
    "    'immigration', 'travel ban', 'border wall',\n",
    "    'roy moore', 'harassment', 'email scandal', 'billy bush',\n",
    "    'global warming', 'paris climate',\n",
    "    'autism', \n",
    "    'planned parenthood', 'abortion',\n",
    "    'gay marriage', 'lgbt',\n",
    "    'fashion', 'wedding',\n",
    "    'facebook', \n",
    "    'irs', 'taxes',\n",
    "    'school', 'preschool',\n",
    "    'nutrition', 'healthcare',\n",
    "    'yoga', 'asthma', 'flu',\n",
    "    'public transportation',\n",
    "    'travel',\n",
    "    'vacation',\n",
    "    'wall street', 'economy', 'trade',\n",
    "    'national security', 'north korea',\n",
    "    'guns', 'education',\n",
    "    'supreme court', 'social security',\n",
    "    'racism', 'afghanistan', 'iraq',\n",
    "    'england', 'europe', 'france',\n",
    "    'football', 'soccer', 'fifa',\n",
    "    'asia', 'africa', 'brazil',\n",
    "    'mexico', 'canada', 'violence',\n",
    "    'internet', 'technology', 'baseball',\n",
    "    'olympics', 'iran', 'crime',\n",
    "    'food', 'disease', 'cancer',\n",
    "    'drugs', 'college', 'police',\n",
    "    'oscars', 'rifle', 'apple',\n",
    "    'blockchain', 'congress',\n",
    "    'president', 'america', 'veteran',\n",
    "    'music', 'film', 'dance',\n",
    "    'book', 'ebola', 'census', 'cars',\n",
    "    'import', 'export', 'christmas', \n",
    "    'july 4th', 'india', 'wine', 'wildfire',\n",
    "    'earthquake', 'flood', 'hurricane', 'jobs',\n",
    "    'luxury goods', 'golf', 'doping',\n",
    "    'ipo', 'bankruptcy', 'literature',\n",
    "    'millennials', 'fiction', 'greece',\n",
    "    'italy', 'liberals', 'conservatives',\n",
    "    'catholic', 'evangelical', 'nafta', \n",
    "    'opiod', 'cats', 'dogs', 'wildlife',\n",
    "    'pets', 'democrat', 'republcian', \n",
    "    'media', 'death penalty', 'meme',\n",
    "    'twitter', 'nasa', 'sports', 'gym', 'medicine',\n",
    "    'affair', 'banks', 'agriculture', 'coal', 'oil',\n",
    "    'renewable energy', 'google', 'gaming', 'artificial intelligence',\n",
    "    'spy', 'wiretap', 'obamacare', 'obesity',\n",
    "    'cyberbullying', 'netflix', 'basketball',\n",
    "    'native american', 'african american', 'hispanic', \n",
    "    'tennis', 'inequality', 'foreign policy', 'charity',\n",
    "    'marijuana', 'childcare', 'insurance', 'vaccine',\n",
    "    'christianity', 'constitution', 'slavery', 'confederate',\n",
    "    'poverty', 'homeless', 'prison', 'patents', 'teens',\n",
    "    'elderly', 'stock market', 'bonds', 'consumers',\n",
    "    'cybersecurity', 'illegal drugs', 'prescription drugs',\n",
    "    'nato', 'freedom of speech', 'freedom of information', 'patriot act',\n",
    "    'due process', 'flint water crisis', 'government shutdown'\n",
    "]\n",
    "\n",
    "SEGMENT_CACHE_PATH = '/tmp/topic_segments'\n",
    "\n",
    "def get_segments(topic):\n",
    "    cache_path = os.path.join(SEGMENT_CACHE_PATH, '{}.pkl'.format(topic))\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        print('Loading {} segments from cache'.format(topic))\n",
    "        return pickle.load(f)\n",
    "\n",
    "topic_to_segments = { t : get_segments(t) for t in topics }\n",
    "for t, s in topic_to_segments.items():\n",
    "    if len(s) == 0:\n",
    "        print('Warning: {} has no segments'.format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_lexicon = {}\n",
    "for t in topics:\n",
    "    with open(os.path.join(LEXICON_CACHE_PATH, '{}.pkl'.format(t)), 'rb') as f:\n",
    "        l = pickle.load(f)\n",
    "        if len(l) == 0:\n",
    "            print('{} has an empty lexicon'.format(t))\n",
    "        else:\n",
    "            topic_to_lexicon[t] = l\n",
    "            \n",
    "TOPIC_LEXICON_PATH = 'widget_data/topic_lexicons.json'\n",
    "if not OVERWRITE and os.path.exists(TOPIC_LEXICON_PATH):\n",
    "    raise Exception('File exists!')\n",
    "    \n",
    "with open(TOPIC_LEXICON_PATH, 'w') as f:\n",
    "    json.dump(topic_to_lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender By Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T22:36:38.342407Z",
     "start_time": "2018-10-07T22:36:08.421317Z"
    }
   },
   "outputs": [],
   "source": [
    "def segments_to_overlapping_face_genders(topic, segments):\n",
    "    intervals_by_video = defaultdict(list)\n",
    "    for video_id, _, interval, _, _ in segments:\n",
    "        intervals_by_video[video_id].append(interval)\n",
    "    face_genders_with_topic_overlap = annotate_interval_overlap(\n",
    "        face_genders, intervals_by_video)\n",
    "    face_genders_with_topic_overlap = face_genders_with_topic_overlap.where(\n",
    "        face_genders_with_topic_overlap.overlap_seconds > 0)\n",
    "    return face_genders_with_topic_overlap\n",
    "\n",
    "topic_to_face_genders = { \n",
    "    t : segments_to_overlapping_face_genders(t, s) \n",
    "    for t, s in topic_to_segments.items() if len(s) > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T02:16:06.686228Z",
     "start_time": "2018-10-07T22:36:38.344789Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_face_genders_all = None\n",
    "for t, df in topic_to_face_genders.items():\n",
    "    df = df.withColumn('topic', func.lit(t))\n",
    "    if topic_face_genders_all is None:\n",
    "        topic_face_genders_all = df\n",
    "    else:\n",
    "        topic_face_genders_all = topic_face_genders_all.union(df) \n",
    "\n",
    "screen_time_male_by_topic_date = {}\n",
    "screen_time_female_by_topic_date = {}\n",
    "screen_time_male_nh_by_topic_date = {}\n",
    "screen_time_female_nh_by_topic_date = {}\n",
    "\n",
    "# Including hosts\n",
    "print('Computing male screen time by topic')\n",
    "for k, v in sum_over_column(\n",
    "    topic_face_genders_all, 'duration', ['topic', 'date'],\n",
    "    probability_column='male_probability'\n",
    ").items():\n",
    "    topic, date = k\n",
    "    screen_time_male_by_topic_date[(topic, date)] = v\n",
    "    \n",
    "print('Computing female screen time by topic')\n",
    "for k, v in sum_over_column(\n",
    "    topic_face_genders_all, 'duration', ['topic', 'date'],\n",
    "    probability_column='female_probability'\n",
    ").items():\n",
    "    topic, date = k\n",
    "    screen_time_female_by_topic_date[(topic, date)] = v\n",
    "\n",
    "# Excluding hosts\n",
    "topic_face_genders_nh_all = topic_face_genders_all.where(topic_face_genders_all.host_probability < 0.5)\n",
    "print('Computing male (non-host) screen time by topic')\n",
    "for k, v in sum_over_column(\n",
    "    topic_face_genders_nh_all, 'duration', ['topic', 'date'],\n",
    "    probability_column='male_probability'\n",
    ").items():\n",
    "    topic, date = k\n",
    "    screen_time_male_nh_by_topic_date[(topic, date)] = v\n",
    "    \n",
    "print('Computing female (non-host) screen time by topic')\n",
    "for k, v in sum_over_column(\n",
    "    topic_face_genders_nh_all, 'duration', ['topic', 'date'],\n",
    "    probability_column='female_probability'\n",
    ").items():\n",
    "    topic, date = k\n",
    "    screen_time_female_nh_by_topic_date[(topic, date)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T02:16:06.823544Z",
     "start_time": "2018-10-08T02:16:06.694572Z"
    }
   },
   "outputs": [],
   "source": [
    "# screen_time_male_by_topic_date = {}\n",
    "# screen_time_female_by_topic_date = {}\n",
    "# screen_time_male_nh_by_topic_date = {}\n",
    "# screen_time_female_nh_by_topic_date = {}\n",
    "\n",
    "# for i, topic in enumerate(topics):\n",
    "#     print(i, topic)\n",
    "#     topic_face_genders = topic_to_face_genders[topic]\n",
    "#     for k, v in sum_over_column(\n",
    "#         topic_face_genders, 'duration', ['date'],\n",
    "#         probability_column='male_probability'\n",
    "#     ).items():\n",
    "#         screen_time_male_by_topic_date[(topic, k[0])] = v\n",
    "\n",
    "#     for k, v in sum_over_column(\n",
    "#         topic_face_genders, 'duration', ['date'],\n",
    "#         probability_column='female_probability'\n",
    "#     ).items():\n",
    "#         screen_time_female_by_topic_date[(topic, k[0])] = v\n",
    "        \n",
    "#     topic_face_genders_nh = topic_face_genders.where(topic_face_genders.host_probability < 0.5)\n",
    "#     for k, v in sum_over_column(\n",
    "#         topic_face_genders_nh, 'duration', ['date'],\n",
    "#         probability_column='male_probability'\n",
    "#     ).items():\n",
    "#         screen_time_male_nh_by_topic_date[(topic, k[0])] = v\n",
    "\n",
    "#     for k, v in sum_over_column(\n",
    "#         topic_face_genders_nh, 'duration', ['date'],\n",
    "#         probability_column='female_probability'\n",
    "#     ).items():\n",
    "#         screen_time_female_nh_by_topic_date[(topic, k[0])] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T02:16:11.606803Z",
     "start_time": "2018-10-08T02:16:06.825786Z"
    }
   },
   "outputs": [],
   "source": [
    "GENDER_BY_TOPIC_PATH = 'widget_data/gender_by_topic.json'\n",
    "if not OVERWRITE and os.path.exists(GENDER_BY_TOPIC_PATH):\n",
    "    raise Exception('File exists!')\n",
    "\n",
    "with open(GENDER_BY_TOPIC_PATH, 'w') as f:\n",
    "    json.dump({\n",
    "        'all': {\n",
    "            'male': json_keys(screen_time_male_by_topic_date),\n",
    "            'female': json_keys(screen_time_female_by_topic_date)\n",
    "        },\n",
    "        'nonhost': {\n",
    "            'male': json_keys(screen_time_male_nh_by_topic_date),\n",
    "            'female': json_keys(screen_time_female_nh_by_topic_date)\n",
    "        }\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T02:19:16.036413Z",
     "start_time": "2018-10-08T02:18:43.007821Z"
    }
   },
   "outputs": [],
   "source": [
    "def segments_to_overlapping_face_identities(topic, segments):\n",
    "    intervals_by_video = defaultdict(list)\n",
    "    for video_id, _, interval, _, _ in segments:\n",
    "        intervals_by_video[video_id].append(interval)\n",
    "    face_identities_with_topic_overlap = annotate_interval_overlap(\n",
    "        face_identities, intervals_by_video)\n",
    "    face_identities_with_topic_overlap = face_identities_with_topic_overlap.where(\n",
    "        face_identities_with_topic_overlap.overlap_seconds > 0)\n",
    "    return face_identities_with_topic_overlap\n",
    "\n",
    "topic_to_face_identities = { \n",
    "    t : segments_to_overlapping_face_identities(t, s) \n",
    "    for t, s in topic_to_segments.items() if len(s) > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T02:19:16.066142Z",
     "start_time": "2018-10-08T02:19:16.039119Z"
    }
   },
   "outputs": [],
   "source": [
    "# OOM on 140 GB!\n",
    "\n",
    "# topic_face_identities_all = None\n",
    "# for t, df in topic_to_face_identities.items():\n",
    "#     df = df.withColumn('topic', func.lit(t))\n",
    "#     if topic_face_identities_all is None:\n",
    "#         topic_face_identities_all = df\n",
    "#     else:\n",
    "#         topic_face_identities_all = topic_face_identities_all.union(df) \n",
    "\n",
    "# screen_time_identity_by_topic_year_month = defaultdict(dict)\n",
    "# for k, v in sum_over_column(\n",
    "#     topic_face_identities_all, 'duration', ['identity_id', 'topic', 'year', 'month'],\n",
    "#     probability_column='probability'\n",
    "# ).items():\n",
    "#     id_id, topic, year, month = k\n",
    "#     if id_id in identity_map:\n",
    "#         screen_time_identity_by_topic_year_month[identity_map[id_id]][(topic, year, month)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T02:19:16.094496Z",
     "start_time": "2018-10-08T02:19:16.068743Z"
    }
   },
   "outputs": [],
   "source": [
    "screen_time_identity_by_topic_date = defaultdict(dict)\n",
    "topics_done = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T03:18:53.915169Z",
     "start_time": "2018-10-08T02:19:16.096897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, topic in enumerate(sorted(topic_to_face_identities)):\n",
    "    print(i, topic)\n",
    "    if topic in topics_done:\n",
    "        continue\n",
    "    topic_face_identities = topic_to_face_identities[topic]\n",
    "    for k, v in sum_over_column(\n",
    "        topic_face_identities, 'duration', ['identity_id', 'date'],\n",
    "        probability_column='probability'\n",
    "    ).items():\n",
    "        id_id, date = k\n",
    "        name = identity_map[id_id]\n",
    "        if id_id in identity_map:\n",
    "            screen_time_identity_by_topic_date[identity_map[id_id]][(topic, date)] = v\n",
    "    topics_done.add(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T03:19:13.383977Z",
     "start_time": "2018-10-08T03:18:53.922012Z"
    }
   },
   "outputs": [],
   "source": [
    "IDENTITY_BY_TOPIC_PATH = 'widget_data/identity_by_topic.json'\n",
    "if not OVERWRITE and os.path.exists(IDENTITY_BY_TOPIC_PATH):\n",
    "    raise Exception('File exists!')\n",
    "    \n",
    "with open(IDENTITY_BY_TOPIC_PATH, 'w') as f:\n",
    "    json.dump({\n",
    "        k : json_keys(v) \n",
    "        for k, v in screen_time_identity_by_topic_date.items() \n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
