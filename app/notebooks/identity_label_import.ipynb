{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T09:31:45.492246Z",
     "start_time": "2019-01-13T09:31:39.465447Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(12, 5)\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from sklearn import metrics\n",
    "\n",
    "from esper.widget import *\n",
    "from esper.prelude import *\n",
    "from esper.plot_util import *\n",
    "import esper.face_embeddings as face_embeddings\n",
    "\n",
    "def split_list(l, idx):\n",
    "    return l[:idx], l[idx:]\n",
    "\n",
    "ReferenceFaces = namedtuple(\n",
    "    'ReferenceFaces', ['name', 'ids', 'embs', 'imgs'])\n",
    "\n",
    "def show_reference_imgs(refs):\n",
    "    tiled_imgs = tile_images(\n",
    "        [cv2.resize(x, (100, 100)) for x in refs.imgs], \n",
    "        cols=10, blank_value=255)\n",
    "    print('Your reference images for {}.'.format(refs.name))\n",
    "    plt.figure()\n",
    "    imshow(tiled_imgs)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T09:31:45.887985Z",
     "start_time": "2019-01-13T09:31:45.496322Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_LABEL = 1\n",
    "NEG_LABEL = 0\n",
    "\n",
    "####\n",
    "# DEBUG PLOTS\n",
    "####\n",
    "\n",
    "def plot_roc(y_true, y_pred, title='Receiver Operating Characteristic'):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_binary_score_histograms(y_true, y_pred, face_references, y_max=None, \n",
    "                                 title='Score Distribution by Class', ):\n",
    "    bins = np.linspace(0, 1, 100)\n",
    "    plt.figure()\n",
    "    plt.hist([x for i, x in enumerate(y_pred) if y_true[i] == POS_LABEL], \n",
    "             bins, alpha=0.5, label=face_references.name)\n",
    "    plt.hist([x for i, x in enumerate(y_pred) if y_true[i] == NEG_LABEL], \n",
    "             bins, alpha=0.5, label='Not {}'.format(face_references.name))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Score')\n",
    "    if y_max is not None: \n",
    "        plt.ylim(0, y_max)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_score_histogram(predictions, sample, x_min=None):\n",
    "    bins = np.linspace(0, 1, 100)\n",
    "    plt.figure()\n",
    "    sampled_pred = (\n",
    "        random.sample(predictions, sample) \n",
    "        if sample < len(predictions) else predictions\n",
    "    )\n",
    "    plt.hist([s for _, s in sampled_pred], bins, alpha=1)\n",
    "    plt.title('Predicted Score Distribution (sample={})'.format(\n",
    "              min(sample, len(predictions))))\n",
    "    plt.xlabel('Predicted Score')\n",
    "    plt.xticks(np.arange(11) / 10)\n",
    "    if x_min is not None:\n",
    "        plt.xlim(left=x_min)\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_estimated_cdf(predictions, sample, x_min=None):\n",
    "    n_bins = 100\n",
    "    def score_to_bin(s):\n",
    "        v = math.ceil(s * n_bins)\n",
    "        return min(v, n_bins)\n",
    "    bins = np.zeros(n_bins + 1)\n",
    "    sampled_pred = (\n",
    "        random.sample(predictions, sample) \n",
    "        if sample < len(predictions) else predictions\n",
    "    )\n",
    "    for _, s in sampled_pred:\n",
    "        bins[score_to_bin(s)] += s\n",
    "\n",
    "    sample_est_pos = np.sum(bins)\n",
    "    total_est_pos = int(sample_est_pos / sample * len(predictions))\n",
    "    \n",
    "    norm_bins = bins / sample_est_pos\n",
    "    cdf_bins = np.cumsum(norm_bins)\n",
    "    inds = np.arange(bins.size) / n_bins\n",
    "    plt.figure()\n",
    "    plt.title('CDF of Positive Predictions ' +\n",
    "              '(total estimated positives={})'.format(\n",
    "              total_est_pos))\n",
    "    plt.plot(inds, cdf_bins, label='Est. Cumulative Proportion')\n",
    "    plt.plot(inds, norm_bins, label='Est. Bin Proportion ({} bins)'.format(n_bins))\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xlabel('Predicted Score')\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xticks(np.arange(11) / 10)\n",
    "    if x_min is not None:\n",
    "        plt.xlim(left=x_min)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Est. Positives by Threshold')\n",
    "    total_est_bins = bins / sample * len(predictions)\n",
    "    total_est_cdf_thresh = np.cumsum(total_est_bins[::-1])\n",
    "    num_thesh = 10\n",
    "    for i in range(num_thesh):\n",
    "        t = 1. - 0.1 * (i + 1)\n",
    "        print('  t={:0.1f}\\t{:0.1f}'.format(\n",
    "              t, total_est_cdf_thresh[num_thesh * (i + 1) - 1]))\n",
    "\n",
    "def train_model(params, face_references, pos_examples, neg_examples, score_threshold, \n",
    "                train_val_ratio=10):\n",
    "    print('Training logistic classifier with {}:1 train to validation split'.format(\n",
    "          train_val_ratio))\n",
    "    \n",
    "    print('Hyperparameters')\n",
    "    print('  Epochs:', params['num_epochs'])\n",
    "    print('  Learning rate:', params['learning_rate'])\n",
    "    print('  L2 penalty:', params['l2_penalty'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pos_examples_copy = list(pos_examples)\n",
    "    random.shuffle(pos_examples_copy)\n",
    "    pos_split_idx = int(len(pos_examples_copy) / train_val_ratio)\n",
    "    val_pos, train_pos = split_list(pos_examples_copy, pos_split_idx)\n",
    "    \n",
    "    neg_examples_copy = list(neg_examples)\n",
    "    random.shuffle(neg_examples_copy)\n",
    "    neg_split_idx = int(len(neg_examples_copy) / train_val_ratio)\n",
    "    val_neg, train_neg = split_list(neg_examples_copy, neg_split_idx)\n",
    "    \n",
    "    train_ids = train_pos + train_neg\n",
    "    train_y = ([POS_LABEL] * len(train_pos)) + ([NEG_LABEL] * len(train_neg))\n",
    "    \n",
    "    val_ids = val_pos + val_neg\n",
    "    val_y = ([POS_LABEL] * len(val_pos)) + ([NEG_LABEL] * len(val_neg))\n",
    "    \n",
    "    weights = face_embeddings.logreg(train_ids, train_y, **params)\n",
    "    predictions = face_embeddings.logreg.logreg_predict(weights, min_thresh=score_threshold)\n",
    "    \n",
    "    model_time = time.time()\n",
    "    print('Trained model and obtained predictions: {:0.4f}s'.format(model_time - start_time))\n",
    "    \n",
    "    train_id_to_idx = {v: i for i, v in enumerate(train_ids)}\n",
    "    train_pred_y = [0] * len(train_ids)\n",
    "    val_id_to_idx = {v: i for i, v in enumerate(val_ids)}\n",
    "    val_pred_y = [0] * len(val_ids)\n",
    "    \n",
    "    for v, s in predictions:\n",
    "        if v in train_id_to_idx:\n",
    "            train_pred_y[train_id_to_idx[v]] = s\n",
    "        if v in val_id_to_idx:\n",
    "            val_pred_y[val_id_to_idx[v]] = s\n",
    "            \n",
    "    num_tabs = 3\n",
    "    outputs = [widgets.Output() for _ in range(num_tabs)]\n",
    "    tabs = widgets.Tab(children=outputs)\n",
    "    \n",
    "    with outputs[0]:\n",
    "        tabs.set_title(0, 'Entire Dataset')\n",
    "        x_min = params.get('min_thresh', None)\n",
    "        if x_min is not None:\n",
    "            print('Minimum score threshold: {}'.format(x_min))\n",
    "        plot_score_histogram(predictions, sample=100000, x_min=x_min)\n",
    "        print('If we interpret the scores produced by the model as probabilities, '\n",
    "              'we can estimate the number of true positives that we expect to find '\n",
    "              'in the dataset. The following plot makes this assumption and shows '\n",
    "              'the expected contribution of faces of varying scores to the total.')\n",
    "        plot_estimated_cdf(predictions, sample=100000, x_min=x_min)\n",
    "        \n",
    "    with outputs[1]:\n",
    "        tabs.set_title(1, 'Training Set')\n",
    "        plot_roc(train_y, train_pred_y)\n",
    "        plot_binary_score_histograms(train_y, train_pred_y, face_references)\n",
    "        \n",
    "    with outputs[2]:\n",
    "        tabs.set_title(2, 'Validation Set')\n",
    "        plot_roc(val_y, val_pred_y)\n",
    "        plot_binary_score_histograms(val_y, val_pred_y, face_references)\n",
    "    \n",
    "    print('Generate debugging plots: {:0.4f}s'.format(time.time() - model_time))\n",
    "    display(tabs)\n",
    "    return weights, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T09:31:45.933790Z",
     "start_time": "2019-01-13T09:31:45.890702Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model_and_examples(path):\n",
    "    print('Loading model: {}'.format(path))\n",
    "    with open(path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    embs = model['init_embs']\n",
    "    ids = set(model['init_ids'])\n",
    "    imgs = model['init_imgs']\n",
    "    weights = model['weights']\n",
    "    \n",
    "    references = ReferenceFaces(\n",
    "        name=model['name'], ids=ids, imgs=imgs, embs=embs)\n",
    "    pos_examples = set(model['pos_examples'])\n",
    "    neg_examples = set(model['neg_examples'])\n",
    "    print('Done! Loaded {} reference faces; {} positive and {} negative examples'.format(\n",
    "          len(embs), len(pos_examples), len(neg_examples)))\n",
    "    return references, pos_examples, neg_examples, weights\n",
    "\n",
    "def list_models(model_dir):\n",
    "    result = []\n",
    "    for fname in os.listdir(model_dir):\n",
    "        result.append(os.path.join(model_dir, fname))\n",
    "    return result\n",
    "\n",
    "NATIVE_MODEL_DIR = '/app/data/identity_models_v2'\n",
    "CONVERTED_MODEL_DIR = '/app/data/identity_models_v2_converted'\n",
    "\n",
    "native_model_files = list_models(NATIVE_MODEL_DIR)\n",
    "print('Found {} native models'.format(len(native_model_files)))\n",
    "\n",
    "converted_model_files = list_models(CONVERTED_MODEL_DIR)\n",
    "print('Found {} converted models'.format(len(converted_model_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T09:31:51.214811Z",
     "start_time": "2019-01-13T09:31:51.167266Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TRANSACTION_SIZE = 100000\n",
    "\n",
    "def run_model(path, labeler_name_prefix, save_to_db=False, score_threshold=0.2):    \n",
    "    face_references, pos_examples, neg_examples, weights = load_model_and_examples(path)\n",
    "    print('Running:', face_references.name)\n",
    "    name = face_references.name.lower()\n",
    "    labeler_name = '{}:{}'.format(labeler_name_prefix, name)\n",
    "    if save_to_db:\n",
    "        existing_label_count = FaceIdentity.objects.filter(\n",
    "            identity__name=name,\n",
    "            labeler__name=labeler_name\n",
    "        ).count()\n",
    "        if existing_label_count > 0:\n",
    "            print('Identities already saved for {} (count={})'.format(\n",
    "                  face_references.name, existing_label_count))\n",
    "            return\n",
    "\n",
    "    show_reference_imgs(face_references)\n",
    "    params = {\n",
    "        'num_epochs': 40,\n",
    "        'learning_rate': 1,\n",
    "        'l2_penalty': 1e-5,\n",
    "    }\n",
    "    weights, predictions = train_model(params, face_references, pos_examples, neg_examples, \n",
    "                                       score_threshold)\n",
    "    print('{} faces passed the threshold'.format(len(predictions)))\n",
    "    \n",
    "    if save_to_db:\n",
    "        labeler, created = Labeler.objects.get_or_create(name=labeler_name)\n",
    "        if created:\n",
    "            print('Created labeler:', labeler_name)\n",
    "        identity, created = Identity.objects.get_or_create(name=name)\n",
    "        if created:\n",
    "            print('Created identity:', name)\n",
    "        valid_face_ids = {\n",
    "            x['id'] for x in \n",
    "            Face.objects.filter(\n",
    "                id__in=[i for i, _ in predictions]\n",
    "            ).values('id')\n",
    "        }\n",
    "        face_identities = []\n",
    "        for i, s in predictions:\n",
    "            if i in valid_face_ids:\n",
    "                face_identities.append(\n",
    "                    FaceIdentity(\n",
    "                        face_id=i, identity=identity, \n",
    "                        probability=s, labeler=labeler))\n",
    "                valid_face_ids.remove(i)\n",
    "        print('Saving {} face identities to the DB. ({} face ids were missing)'.format(\n",
    "              len(face_identities), len(predictions) - len(face_identities)))\n",
    "        saved_count = 0\n",
    "        for i in range(0, len(face_identities), MAX_TRANSACTION_SIZE):\n",
    "            face_identities_subset = face_identities[i:i + MAX_TRANSACTION_SIZE]\n",
    "            FaceIdentity.objects.bulk_create(face_identities_subset)\n",
    "            saved_count += len(face_identities_subset)\n",
    "            print('  saved {} / {}'.format(saved_count, len(face_identities)))\n",
    "        print('Done!')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T00:22:03.407371Z",
     "start_time": "2019-01-10T23:56:54.503167Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SAVE_TO_DB = True\n",
    "NATIVE_LABELER_NAME_PREFIX = 'face-identity'\n",
    "\n",
    "for model_path in sorted(native_model_files):\n",
    "    run_model(model_path, NATIVE_LABELER_NAME_PREFIX, save_to_db=SAVE_TO_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T10:32:22.907627Z",
     "start_time": "2019-01-06T06:49:23.063646Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_TO_DB = True\n",
    "CONV_LABELER_NAME_PREFIX = 'face-identity-converted'\n",
    "\n",
    "def get_name_from_path(p):\n",
    "    return p.split('/')[-1].split('.')[0].lower()\n",
    "\n",
    "for model_path in sorted(converted_model_files):\n",
    "    name = get_name_from_path(model_path).replace('_', ' ')\n",
    "    if Labeler.objects.filter(name='face-identity:{}'.format(name)).count() > 0:\n",
    "        print('Native labels already exist for {}. Skipping import.'.format(name))\n",
    "    else:\n",
    "        run_model(model_path, CONV_LABELER_NAME_PREFIX, save_to_db=SAVE_TO_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T09:55:41.643709Z",
     "start_time": "2019-01-13T09:34:11.847069Z"
    }
   },
   "outputs": [],
   "source": [
    "SAVE_TO_DB = True\n",
    "NATIVE_LABELER_NAME_PREFIX = 'face-identity'\n",
    "\n",
    "for model_path in sorted([\n",
    "    '/app/data/identity_models_v2/mika_brzezinski.pkl', \n",
    "    '/app/data/identity_models_v2/willie_geist.pkl',\n",
    "    '/app/data/identity_models_v2/joe_scarborough.pkl'\n",
    "]):\n",
    "    run_model(model_path, NATIVE_LABELER_NAME_PREFIX, save_to_db=SAVE_TO_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
